@article{e262fe0ee530460793e3dc3bf45b1c65,
  title = {TabID: Automatic Identification and Tabulation of Subproblems in Constraint Models},
  abstract = {The performance of a constraint model can often be improved by converting a subproblem into a single table constraint (referred to as tabulation). Finding subproblems to tabulate is traditionally a manual and time-intensive process, even for expert modellers. This paper presents TabID, an entirely automated method to identify promising subproblems for tabulation in constraint programming. We introduce a diverse set of heuristics designed to identify promising candidates for tabulation, aiming to improve solver performance.These heuristics are intended to encapsulate various factors that contribute to useful tabulation.We also present additional checks to limit the potential drawbacks of suboptimal tabulation. We comprehensively evaluate our approach using benchmark problems from existing literature that previously relied on manual identification by constraint programming experts of constraints to tabulate.We demonstrate that our automated identification and tabulation process achieves comparable, and in some cases improved results. We empirically evaluate the efficacy of our approach on a variety of solvers, including standard CP (Minion and Gecode), clause-learning CP (Chuffed and OR-Tools) and SAT solvers (Kissat).Our findings highlight the substantial potential of fully automated tabulation, suggesting its integration into automated model reformulation tools.},
  author = {Ozgur Akgun and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Zeynep Kiziltan and Miguel, {Ian James} and Peter Nightingale and Salamon, {Andr{\'a}s Z.} and Felix Ulrich-Oltean},
  note = {Funding: We thank EPSRC for grants numbered EP/P015638/1, EP/P026842/1, EP/R513386/1, EP/W001977/1, and EP/V027182/1 which have supported the authors at various times while this research was undertaken. Dr Jefferson held a Royal Society University Research Fellowship.},
  year = {2025},
  month = {mar},
  day = {30},
  doi = {10.1613/jair.1.17032},
  language = {English},
  volume = {82},
  pages = {1999--2056},
  journal = {Journal of Artificial Intelligence Research},
  issn = {1076-9757},
  publisher = {Morgan Kaufmann Publishers, Inc.},
}

@article{14008ff0c1c646c2938de12438a94857,
  title = {Composable Constraint Models for Permutation Enumeration},
  abstract = {Constraint programming (CP) is a powerful tool for modeling mathematical concepts and objects and finding both solutions or counter examples. One of the major strengths of CP is that problems can easily be combined or expanded. In this paper, we illustrate that this versatility makes CP an ideal tool for exploring problems in permutation patterns. We declaratively define permutation properties, permutation pattern avoidance and containment constraints using CP and show how this allows us to solve a wide range of problems. We show how this approach enables the arbitrary composition of these conditions, and also allows the easy addition of extra conditions. We demonstrate the effectiveness of our techniques by modelling the containment and avoidance of six permutation patterns, eight permutation properties and measuring five statistics on the resulting permutations. In addition to calculating properties and statistics for the generated permutations, we show that arbitrary additional constraints can also be easily and efficiently added. This approach enables mathematicians to investigate permutation pattern problems in a quick and efficient manner. We demonstrate the utility of constraint programming for permutation patterns by showing how we can easily and efficiently extend the known permutation counts for a conjecture involving the class of 1324 avoiding permutations. For this problem, we expand the enumeration of 1324-avoiding permutations with a fixed number of inversions to permutations of length 16 and show for the first time that in the enumeration there is a pattern occurring which follows a unique sequence on the Online Encyclopedia of Integer Sequences.},
  keywords = {Constraint modelling, Permutation pattern, Enumeration},
  author = {Ruth Hoffmann and Ozgur Akgun and Christopher Jefferson},
  year = {2025},
  month = {jan},
  day = {22},
  doi = {10.46298/dmtcs.12620},
  language = {English},
  volume = {26},
  journal = {Discrete Mathematics & Theoretical Computer Science},
  issn = {1365-8050},
  number = {1},
}

@inproceedings{ab15784dc0d340fc9e8a10be0e6aa935,
  title = {Breaking the Symmetries of Indistinguishable Objects},
  keywords = {Symmetries, Modelling, Constraint programming, Automated model transformation},
  author = {Ozgur Akgun and Chang, {Mun See} and Gent, {Ian Philip} and Jefferson, {Christopher Anthony}},
  note = {Funding: This work was supported by the Engineering and Physical Sciences Research Council EP/Y000609/1.; The 22nd International Conference on the Integration of Constraint Programming, Artificial Intelligence,<br/>and Operations Research (CPAIOR 2025), CPAIOR ; Conference date: 10-11-2025 Through 13-11-2025},
  year = {2025},
  language = {English},
  series = {Lecture notes in computer science},
  publisher = {Springer},
  booktitle = {The Twenty-Second International Conference of the Integration of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR 2025)},
  address = {Netherlands},
  url = {https://sites.google.com/view/cpaior2025},
}

@inproceedings{d26b126cf5e44925bd637f45e87c046c,
  title = {Automatic Feature Learning for Essence: a Case Study on Car Sequencing},
  abstract = {Constraint modelling languages such as Essence offer a means to describe combinatorial problems at a high-level, i.e., without committing to detailed modelling decisions for a particular solver or solving paradigm. Given a problem description written in Essence, there are multiple ways to translate it to a low-level constraint model. Choosing the right combination of a low-level constraint model and a target constraint solver can have significant impact on the effectiveness of the solving process. Furthermore, the choice of the best combination of constraint model and solver can be instance-dependent, i.e., there may not exist a single combination that works best for all instances of the same problem. In this paper, we consider the task of building machine learning models to automatically select the best combination for a problem instance. A critical part of the learning process is to define instance features, which serve as input to the selection model. Our contribution is automatic learning of instance features directly from the high-level representation of a problem instance using a language model. We evaluate the performance of our approach using the Essence modelling language with a case study involving the car sequencing problem.},
  keywords = {Constraint modelling, Algorithm selection, Feature extraction, Machine learning, Language model},
  author = {Alessio Pellegrino and {\"O}zg{\"u}r Akg{\"u}n and Nguyen Dang and Zeynep Kiziltan and Ian Miguel},
  note = {Funding: This work was supported by the European Union{\textquoteright}s Justice programme, under GA No 101087342, POLINE (Principles Of Law In National and European VAT) and by a scholarship from the Department of Computer Science and Engineering of the University of Bologna.; The 23rd workshop on Constraint Modelling and Reformulation (ModRef 2024), ModRef 2024 ; Conference date: 02-09-2024 Through 02-09-2024},
  year = {2024},
  month = {sep},
  day = {23},
  booktitle = {The Twenty-Third Workshop on Constraint Modelling and Reformulation (ModRef 2024)},
  language = {English},
  url = {https://modref.github.io/ModRef2024.html},
}

@inproceedings{51bc896f7bec4383a496391ae598dc3f,
  title = {Automated Nogood-Filtered Fine-Grained Streamlining: A Case Study on Covering Arrays},
  abstract = {We present an automated method to enhance constraint models through fine-grained streamlining, leveraging no good information from learning solvers. This approach reformulates the streamlining process by filtering streamliners based on nogood data from the SAT solver CaDiCaL. Our method generates candidate streamliners from high-level Essence specifications, constructs a streamliner portfolio using Monte Carlo Tree Search, and applies these to unseen problem instances. The key innovation lies in utilising learnt clauses to guide streamliner filtering, effectively reformulating the original model to focus on areas of high search activity. We demonstrate our approach on the Covering Array Problem, achieving significant speedup compared to the state-of-the-art coarse-grained method. This work not only enhances solver efficiency but also provides new insights into automated model reformulation, with potential applications across a wide range of constraint satisfaction problems.},
  keywords = {Constraint programming, Constraint modelling, Constraint satisfaction problem},
  author = {Yazicilar, {Orhan Yigit} and Ozgur Akgun and Miguel, {Ian James}},
  year = {2024},
  month = {sep},
  day = {2},
  booktitle = {The Twenty-Third Workshop on Constraint Modelling and Reformulation (ModRef 2024)},
  language = {English},
  url = {https://modref.github.io/ModRef2024.html},
}

@inproceedings{babbd3f4686a4a4ea13ff79b6207eca5,
  title = {Frugal Algorithm Selection},
  abstract = {When solving decision and optimisation problems, many competing algorithms (model and solver choices) have complementary strengths. Typically, there is no single algorithm that works well for all instances of a problem. Automated algorithm selection has been shown to work very well for choosing a suitable algorithm for a given instance. However, the cost of training can be prohibitively large due to running candidate algorithms on a representative set of training instances. In this work, we explore reducing this cost by choosing a subset of the training instances on which to train. We approach this problem in three ways: using active learning to decide based on prediction uncertainty, augmenting the algorithm predictors with a timeout predictor, and collecting training data using a progressively increasing timeout. We evaluate combinations of these approaches on six datasets from ASLib and present the reduction in labelling cost achieved by each option.},
  keywords = {Active learning, Algorithm Selection},
  author = {Erdem Kus and Ozgur Akgun and Miguel, {Ian James} and Nguyen Dang},
  year = {2024},
  month = {aug},
  day = {29},
  doi = {10.4230/LIPIcs.CP.2024.38},
  language = {English},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {Dagstuhl Publishing},
  editor = {Paul Shaw},
  booktitle = {The Thirtieth International Conference on Principles and Practice of Constraint Programming (CP 2024)},
  note = {30th International Conference on Principles and Practice of Constraint Programming, CP 2024 ; Conference date: 02-09-2024 Through 06-09-2024},
  url = {https://cp2024.a4cp.org/},
}

@conference{e23fb59f2d2447c3b851bbb507b7c099,
  title = {Cost-Efficient Training for Automated Algorithm Selection},
  abstract = {When solving decision and optimisation problems, many competing algorithms have complementary strengths. Typically, there is no single algorithm that works well for all instances of a problem. Automated algorithm selection has been shown to work very well for choosing a suitable algorithm for a given instance. However, the cost of training can be prohibitively large due to the need of running all candidate algorithms on a set of training instances. In this work, we explore reducing this cost by selecting specific instance/algorithm combinations to train on, rather than requiring all algorithms for all instances. We approach this problem in three ways: using active learning to decide based on prediction uncertainty, augmenting the algorithm predictors with a timeout predictor, and collecting training data using a progressively increasing timeout. We evaluate combinations of these approaches on six datasets from ASLib and present the reduction in labelling cost.},
  keywords = {Automated Algorithm Selection, Active Learning, Constraint Programming},
  author = {Erdem Kus and Miguel, {Ian James} and Ozgur Akgun and Nguyen Dang},
  year = {2024},
  month = {jul},
  day = {12},
  language = {English},
  booktitle = {International Conference on Automated Machine Learning (AUTOML 24)},
  note = {International Conference on Automated Machine Learning, AUTOML24 ; Conference date: 09-09-2024 Through 12-09-2024},
  url = {https://2024.automl.cc/},
}

@article{5e48e2440f804d26b33a20c4e72e9ff3,
  title = {Solvi: A visual constraint modeling tool},
  abstract = {Discrete constraint problems surface often in everyday life. Teachers might group students with complex considerations and hospital administrators need to produce staff rosters. Constraint programming (CP) provides techniques to efficiently find solutions. However, there remains a key challenge: these techniques are still largely inaccessible because expressing constraint problems requires sophisticated programming and logic skills. In this work we contribute a language and tool that leverage knowledge of how non-experts conceptualize problems to facilitate the expression of constraint models. Additionally, we report the results of a study surveying the advantages and remaining challenges towards making CP accessible to the wider public.},
  keywords = {Constraints programming, Visualization, Visual modeling, Visual language, human computer interaction},
  author = {Xu Zhu and Miguel Nacenta and Ozgur Akgun and Daniel Zenkovitch},
  note = {Current Funding Sources List: Natural Sciences and Engineering Research Council of Canada, Canada Award Number: 2020-04401 — Recipient: Miguel A Nacenta. Engineering and Physical Sciences Research Council, United Kingdom Award Number: DTG1796157 — Recipient: Xu Zhu.},
  year = {2024},
  month = {mar},
  doi = {10.1016/j.cola.2023.101242},
  language = {English},
  volume = {78},
  journal = {Journal of Computer Languages},
  url = {https://www.sciencedirect.com/science/article/pii/S2590118423000527},
  issn = {2665-9182},
  publisher = {Elsevier},
}

@inproceedings{79fdf668ac41414ea3542907ce49e71a,
  title = {Learning when to use automatic tabulation in constraint model reformulation},
  abstract = {Combinatorial optimisation has numerous practical applications, such as planning, logistics, or circuit design. Problems such as these can be solved by approaches such as Boolean Satisfiability (SAT) or Constraint Programming (CP). Solver performance is affected significantly by the model chosen to represent a given problem, which has led to the study of model reformulation. One such method is tabulation: rewriting the expression of some of the model constraints in terms of a single “table” constraint. Successfully applying this process means identifying expressions amenable to trans- formation, which has typically been done manually. Recent work introduced an automatic tabulation using a set of hand-designed heuristics to identify constraints to tabulate. However, the performance of these heuristics varies across problem classes and solvers. Recent work has shown learning techniques to be increasingly useful in the context of automatic model reformulation. The goal of this study is to understand whether it is possible to improve the performance of such heuristics, by learning a model to predict whether or not to activate them for a given instance. Experimental results suggest that a random forest classifier is the most robust choice, improving the performance of four different SAT and CP solvers.},
  keywords = {Constraint Satisfaction and Optimization: CSO: Modeling, Constraint Satisfaction and Optimization: CSO: Solvers and tools, Machine Learning: ML: Classification},
  author = {Carlo Cena and Ozgur Akgun and Zeynep Kiziltan and Miguel, {Ian James} and Peter Nightingale and Felix Ulrich-Oltean},
  note = {Funding Information: We are grateful for the computational support from the University of York HPC service, Viking and the Research Computing team. This work was supported by EPSRC grants EP/R513386/1, EP/V027182/1 and EP/W001977/1 and by a scholarship from the Department of Computer Science and Engineering of the University of Bologna. Publisher Copyright: {\textcopyright} 2023 International Joint Conferences on Artificial Intelligence. All rights reserved.; 32nd International Joint Conference on Artificial Intelligence, IJCAI 2023 ; Conference date: 19-08-2023 Through 25-08-2023},
  year = {2023},
  month = {aug},
  day = {25},
  doi = {10.24963/ijcai.2023/211},
  language = {English},
  series = {IJCAI International Joint Conference on Artificial Intelligence},
  publisher = {International Joint Conferences on Artificial Intelligence},
  pages = {1902--1910},
  editor = {Edith Elkind},
  booktitle = {The Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI 2023)},
}

@inproceedings{b0595c37d1cb4bca8819f5f38b367dc5,
  title = {Conjure: Automatic generation of constraint models from problem specifications (extended abstract)},
  abstract = {When solving a combinatorial problem, the formulation or model of the problem is critical to the efficiency of the solver. Automating the modelling process has long been of interest given the expertise and time required to develop an effective model of a particular problem. We describe a method to automatically produce constraint models from a problem specification written in the abstract constraint specification language ESSENCE. Our approach is to incrementally refine the specification into a concrete model by applying a chosen refinement rule at each step. Any non-trivial specification may be refined in multiple ways, creating a diverse space of models to choose from. The handling of symmetries is a particularly important aspect of automated modelling. We show how modelling symmetries may be broken automatically as they enter a model during refinement, removing the need for an expensive symmetry detection step following model formulation. Our approach is implemented in a system called CONJURE. We compare the models produced by CONJURE to constraint models from the literature that are known to be effective. Our empirical results confirm that CONJURE can reproduce successfully the kernels of the constraint models of 42 benchmark problems found in the literature.},
  keywords = {Constraint satisfaction and optimization: CSO: constraint programming, Constraint satisfaction and optimization: CSO: modeling},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Frisch, {Alan M.} and Gent, {Ian P.} and Christopher Jefferson and Ian Miguel and Peter Nightingale},
  note = {Funding: This work is supported by the EPSRC grants EP/P015638/1 and EP/P026842/1.; 32nd International Joint Conference on Artificial Intelligence, IJCAI 2023 ; Conference date: 19-08-2023 Through 25-08-2023},
  year = {2023},
  month = {aug},
  day = {19},
  doi = {10.24963/ijcai.2023/765},
  language = {English},
  series = {IJCAI International Joint Conference on Artificial Intelligence},
  publisher = {International Joint Conferences on Artificial Intelligence},
  pages = {6833--6838},
  editor = {Edith Elkind},
  booktitle = {The Thirty-Second International Joint Conference on Artificial Intelligence (IJCAI 2023)},
  url = {https://www.ijcai.org/proceedings/2023/765},
}

@conference{2b9edbeb73734a3c9f11eb835de4a9c1,
  title = {An Approach to Population Linkage using Graph Databases},
  abstract = {We report on a database project which is in the process of linking 29 million vital event records encompassing the entire population of Scotland from 1856 until 1973. Since these records contain no common identifiers, the challenge is to form a pedigree by performing probabilistic linkage over the records. We describe the linkage methodology used to create links between records, for example identifying the birth and marriage records of a single person, and discuss the database technologies employed in the project. A graph database (Neo4j) is used to store both the original vital event records and the links made between them. A metric index is used to find potential links efficiently. Finally, we demonstrate how linkage can be improved by augmenting links based on record distance thresholds with local graph analysis.},
  keywords = {Metric indexing, Metric search, Data linkage, Graph databases, Similarity search},
  author = {Alan Dearle and Kirby, {Graham Njal Cameron} and Ozgur Akgun},
  note = {Funding: This work was supported by ESRC grants ES/K00574X/2 “Digitising Scotland”, ES/L007487/1 “Administrative Data Research Centre – Scotland”, ES/S007407/1 “Administrative Data Research Centres 2018” and ES/W010321/1 “2022-2026 ADR UK Programme”.; 31st Italian Symposium on Advanced Database Systems, SEBD 2023 ; Conference date: 02-07-2023 Through 05-07-2023},
  year = {2023},
  month = {jul},
  day = {5},
  language = {English},
  pages = {291--302},
  booktitle = {The Thirty-First Italian Symposium on Advanced Database Systems (SEBD 2023)},
  note = {31st Italian Symposium on Advanced Database Systems, SEBD 2023 ; Conference date: 02-07-2023 Through 05-07-2023},
  url = {https://sebd2023.dei.unipd.it},
}

@article{c5cc8a3b23e049a18d94d62e720c5edb,
  title = {Automated Streamliner Portfolios for Constraint Satisfaction Problems},
  abstract = {Constraint Programming (CP) is a powerful technique for solving large-scale combinatorial problems. Solving a problem proceeds in two distinct phases: modelling and solving. Effective modelling has a huge impact on the performance of the solving process. Even with the advance of modern automated modelling tools, search spaces involved can be so vast that problems can still be difficult to solve. To further constrain the model, a more aggressive step that can be taken is the addition of streamliner constraints, which are not guaranteed to be sound but are designed to focus effort on a highly restricted but promising portion of the search space. Previously, producing effective streamlined models was a manual, difficult and time-consuming task. This paper presents a completely automated process to the generation, search and selection of streamliner portfolios to produce a substantial reduction in search effort across a diverse range of problems. The results demonstrate a marked improvement in performance for both Chuffed, a CP solver with clause learning, and lingeling, a modern SAT solver.},
  keywords = {Constraint programming, Constraint modelling, Constraint satisfaction problem, Algorithm selection},
  author = {Spracklen, {Justin Lewis Patrick John} and Nguyen Dang and Ozgur Akgun and Miguel, {Ian James}},
  note = {Funding: This work is supported by the EPSRC grants EP/P015638/1 and EP/P026842/1, and Nguyen Dang is a Leverhulme Early Career Fellow. The authors used the Cirrus UK National Tier-2 HPC Service at EPCC (http://www.cirrus.ac.uk) funded by the University of Edinburgh and EPSRC (EP/P020267/1).},
  year = {2023},
  month = {jun},
  day = {1},
  doi = {10.1016/j.artint.2023.103915},
  language = {English},
  volume = {319},
  journal = {Artificial Intelligence},
  issn = {0004-3702},
  publisher = {Elsevier},
}

@article{ba536fe7d91a49a8924844c107ffd83e,
  title = {Conjure: Automatic Generation of Constraint Models from Problem Specifications},
  abstract = {When solving a combinatorial problem, the formulation or model of the problem is critical to the efficiency of the solver. Automating the modelling process has long been of interest because of the expertise and time required to produce an effective model of a given problem. We describe a method to automatically produce constraint models from a problem specification written in the abstract constraint specification language Essence. Our approach is to incrementally refine the specification into a concrete model by applying a chosen refinement rule at each step. Any non-trivial specification may be refined in multiple ways, creating a space of models to choose from.The handling of symmetries is a particularly important aspect of automated modelling. Many combinatorial optimisation problems contain symmetry, which can lead to redundant search. If a partial assignment is shown to be invalid, we are wasting time if we ever consider a symmetric equivalent of it. A particularly important class of symmetries are those introduced by the constraint modelling process: modelling symmetries. We show how modelling symmetries may be broken automatically as they enter a model during refinement, obviating the need for an expensive symmetry detection step following model formulation.Our approach is implemented in a system called Conjure. We compare the models produced by Conjure to constraint models from the literature that are known to be effective. Our empirical results confirm that Conjure can reproduce successfully the kernels of the constraint models of 42 benchmark problems found in the literature.},
  keywords = {Constraint satisfaction problem, Constraint modelling, Constraint programming, Combinatorial optimization},
  author = {Ozgur Akgun and Frisch, {Alan M.} and Gent, {Ian P.} and Christopher Jefferson and Miguel, {Ian J.} and Peter Nightingale},
  note = {Funding: Engineering and Physical Sciences Research Council (EP/V027182/1, EP/P015638/1), Royal Society (URF/R/180015).},
  year = {2022},
  month = {sep},
  day = {1},
  doi = {10.1016/j.artint.2022.103751},
  language = {English},
  volume = {310},
  journal = {Artificial Intelligence},
  issn = {0004-3702},
  publisher = {Elsevier},
}

@inproceedings{c341e68df4c14ab5b77b0693aa1cc90f,
  title = {A framework for generating informative benchmark instances},
  abstract = {Benchmarking is an important tool for assessing the relative performance of alternative solving approaches. However, the utility of benchmarking is limited by the quantity and quality of the available problem instances. Modern constraint programming languages typically allow the specification of a class-level model that is parameterised over instance data. This separation presents an opportunity for automated approaches to generate instance data that define instances that are graded (solvable at a certain difficulty level for a solver) or can discriminate between two solving approaches. In this paper, we introduce a framework that combines these two properties to generate a large number of benchmark instances, purposely generated for effective and informative benchmarking. We use five problems that were used in the MiniZinc competition to demonstrate the usage of our framework. In addition to producing a ranking among solvers, our framework gives a broader understanding of the behaviour of each solver for the whole instance space; for example by finding subsets of instances where the solver performance significantly varies from its average performance.},
  keywords = {Instance generation, Benchmarking, Constraint programming},
  author = {Nguyen Dang and Ozgur Akgun and {Espasa Arxer}, Joan and Miguel, {Ian James} and Peter Nightingale},
  note = {Funding: Nguyen Dang: is a Leverhulme Early Career Fellow; Ian Miguel: supported by EPSRC EP/V027182/1.},
  year = {2022},
  month = {jul},
  day = {23},
  doi = {10.4230/LIPIcs.CP.2022.18},
  language = {English},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {Dagstuhl Publishing},
  editor = {Christine Solon},
  booktitle = {The Twenty-Eighth International Conference on Principles and Practice of Constraint Programming (CP 2022)},
}

@inproceedings{1436930e0d004c7fa6adee33e4c98bd6,
  title = {Understanding how people approach constraint modelling and solving},
  abstract = {Research in constraint programming typically focuses on problem solving efficiency. However, the way users conceptualise problems and communicate with constraint programming tools is often sidelined. How humans think about constraint problems can be important for the development of efficient tools that are useful to a broader audience. For example, a system incorporating knowledge on how people think about constraint problems can provide explanations to users and improve the communication between the human and the solver.We present an initial step towards a better understanding of the human side of the constraint solving process. To our knowledge, this is the first human-centred study addressing how people approach constraint modelling and solving. We observed three sets of ten users each (constraint programmers, computer scientists and non-computer scientists) and analysed how they find solutions for well-known constraint problems. We found regularities offering clues about how to design systems that are more intelligible to humans.},
  keywords = {Constraint modelling, HCI, User study, Grounded theory},
  author = {Ruth Hoffmann and Xu Zhu and Ozgur Akgun and Miguel Nacenta},
  note = {Funding: This work is partially funded by NSERC Discovery Grant 2020-04401 (Canada). Xu Zhu: University of St Andrews and EPSRC grant DTG 1796157. ; The Twenty-Eighth International Conference on Principles and Practice of Constraint Programming (CP 2022), CP 2022 ; Conference date: 31-07-2022 Through 05-08-2022},
  year = {2022},
  month = {jul},
  day = {23},
  doi = {10.4230/LIPIcs.CP.2022.28},
  language = {English},
  series = {Leibniz International Proceedings in Informatics (LIPIcs)},
  publisher = {Dagstuhl Publishing},
  editor = {Christine Solnon},
  booktitle = {The Twenty-Eighth International Conference on Principles and Practice of Constraint Programming (CP 2022)},
  url = {https://cp2022.a4cp.org/},
}

@article{10da0af044e1456383ca5e04ab7d7cb3,
  title = {Enumeration of set-theoretic solutions to the Yang-Baxter equation},
  abstract = {We use Constraint Satisfaction methods to enumerate and construct set-theoretic solutions to the Yang-Baxter equation of small size. We show that there are 321931 involutive solutions of size nine, 4895272 involutive solutions of size ten and 422449480 non-involutive solution of size eight. Our method is then used to enumerate non-involutive biquandles. },
  keywords = {math.GR, math.QA, 16T25},
  author = {Özgür Akgün and Martín Mereb and Leandro Vendramin},
  note = {Funding: The second author is partially supported by PICT 2018-3511 and is also a Junior Associate of the ICTP. The third author acknowledges support of NYU-ECNU Institute of Mathematical Sciences at NYU–Shanghai and he is supported in part by PICT 2016-2481 and UBACyT 20020170100256BA.},
  year = {2022},
  month = {jan},
  day = {14},
  doi = {10.1090/mcom/3696},
  url = {https://www.ams.org/journals/mcom/0000-000-00/S0025-5718-2022-03696-6/},
  language = {English},
  volume = {Early View},
  journal = {Mathematics of Computation},
  issn = {0025-5718},
  publisher = {American Mathematical Society},
}

@inproceedings{36323c314240451291f8377d5a4f9668,
  title = {Towards reformulating Essence specifications for robustness},
  abstract = {The Essence language allows a user to specify a constraint problem at a level of abstraction above that at which constraint modelling decisions are made. Essence specifications are refined into constraint models using the Conjure automated modelling tool, which employs a suite of refinement rules. However, Essence is a rich language in which there are many equivalent ways to specify a given problem. A user may therefore omit the use of domain attributes or abstract types, resulting in fewer refinement rules being applicable and therefore a reduced set of output models from which to select. This paper addresses the problem of recovering this information automatically to increase the robustness of the quality of the output constraint models in the face of variation in the input Essence specification. We present reformulation rules that can change the type of a decision variable or add attributes that shrink its domain. We demonstrate the efficacy of this approach in terms of the quantity and quality of models Conjure can produce from the transformed specification compared with the original. },
  author = {{\"O}zg{\"u}r Akg{\"u}n and Frisch, {Alan M.} and Gent, {Ian P.} and Christopher Jefferson and Ian Miguel and Peter Nightingale and Salamon, {Andr{\'a}s Z.}},
  note = {Funding: This research was supported by the UK EPSRC grants EP/K015745/1 and EP/V027182/1. Chris Jefferson is a University Research Fellow funded by the Royal Society; The 20th workshop on Constraint Modelling and Reformulation (ModRef), ModRef ; Conference date: 25-10-2021 Through 25-10-2021},
  year = {2021},
  month = {oct},
  day = {25},
  language = {English},
  booktitle = {The Twentieth Workshop on Constraint Modelling and Reformulation (ModRef 2021)},
  url = {https://modref.github.io/ModRef2021.html},
}

@inproceedings{d586090d1c36443da26e5656438fc8ab,
  title = {Finding subgraphs with side constraints},
  abstract = {The subgraph isomorphism problem is to find a small “pattern” graph inside a larger “target” graph. There are excellent dedicated solvers for this problem, but they require substantial programming effort to handle the complex side constraints that often occur in practical applications of the problem; however, general purpose constraint solvers struggle on more difficult graph instances. We show how to combine the state of the art Glasgow Subgraph Solver with the Minion constraint programming solver to get a “subgraphs modulo theories” solver that is both performant and flexible. We also show how such an approach can be driven by the Essence high level modelling language, giving ease of modelling and prototyping to non-expert users. We give practical examples involving temporal graphs, typed graphs from software engineering, and costed subgraph isomorphism problems.},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Jessica Enright and Christopher Jefferson and Ciaran McCreesh and Patrick Prosser and Steffen Zschaler},
  note = {Funding: This research was supported by the Engineering and Physical Sciences Research Council [grant number EP/P026842/1].; 18th International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research, CPAIOR 2021 ; Conference date: 05-07-2021 Through 08-07-2021},
  year = {2021},
  month = {jul},
  day = {5},
  doi = {10.1007/978-3-030-78230-6_22},
  language = {English},
  isbn = {9783030782290},
  series = {Lecture notes in computer science (including subseries Lecture notes in artificial intelligence and Lecture notes in bioinformatics)},
  publisher = {Springer},
  pages = {348--364},
  editor = {Stuckey, {Peter J.}},
  booktitle = {The Eighteenth International Conference of the Integration of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR 2021)},
  address = {Netherlands},
}

@inproceedings{ae7faa006d3d4ecbb19a14df924e2088,
  title = {Efficient incremental modelling and solving},
  abstract = {In various scenarios, a single phase of modelling and solving is either not sufficient or not feasible to solve the problem at hand. A standard approach to solving AI planning problems, for example, is to incrementally extend the planning horizon and solve the problem of trying to find a plan of a particular length. Indeed, any optimization problem can be solved as a sequence of decision problems in which the objective value is incrementally updated. Another example is constraint dominance programming (CDP), in which search is organized into a sequence of levels. The contribution of this work is to enable a native interaction between SAT solvers and the automated modelling system Savile Row to support efficient incremental modelling and solving. This allows adding new decision variables, posting new constraints and removing existing constraints (via assumptions) between incremental steps. Two additional benefits of the native coupling of modelling and solving are the ability to retain learned information between SAT solver calls and to enable SAT assumptions, further improving flexibility and efficiency. Experiments on one optimisation problem and five pattern mining tasks demonstrate that the native interaction between the modelling system and SAT solver consistently improves performance significantly. },
  keywords = {Constraint programming, Constraint modelling, Incremental solving, Constraint optimization, Planning, Data mining, Itemset mining, Pattern mining, Dominance programming},
  author = {G{\"o}kberk Ko{\c c}ak and {\"O}zg{\"u}r Akg{\"u}n and Nguyen Dang and Ian Miguel},
  note = {Funding: This work is supported by EPSRC grant EP/P015638/1. Nguyen Dang is a Leverhulme Trust Early Career Fellow (ECF-2020-168).; The 19th workshop on Constraint Modelling and Reformulation (ModRef), ModRef ; Conference date: 07-09-2020 Through 07-09-2020},
  year = {2020},
  month = {sep},
  day = {7},
  language = {English},
  booktitle = {The Nineteenth Workshop on Constraint Modelling and Reformulation (ModRef 2020)},
  url = {https://modref.github.io/ModRef2020.html},
}

@inproceedings{c651056c471d41e3af7df7f4ca9b23ec,
  title = {Exploring instance generation for automated planning},
  abstract = {Many of the core disciplines of artificial intelligence have sets of standard benchmark problems well known and widely used by the community when developing new algorithms. Constraint programming and automated planning are examples of these areas, where the behaviour of a new algorithm is measured by how it performs on these instances. Typically the efficiency of each solving method varies not only between problems, but also between instances of the same problem. Therefore, having a diverse set of instances is crucial to be able to effectively evaluate a new solving method. Current methods for automatic generation of instances for Constraint Programming problems start with a declarative model and search for instances with some desired attributes, such as hardness or size. We first explore the difficulties of adapting this approach to generate instances starting from problem specifications written in PDDL, the de-facto standard language of the automated planning community. We then propose a new approach where the whole planning problem description is modelled using Essence, an abstract modelling language that allows expressing high-level structures without committing to a particular low level representation in PDDL. },
  author = {{\"O}zg{\"u}r Akg{\"u}n and Nguyen Dang and Joan Espasa and Ian Miguel and Salamon, {Andr{\'a}s Z.} and Christopher Stone},
  note = {Funding: This work is supported by EPSRC grant EP/P015638/1. Nguyen Dang is a Leverhulme Early Career Fellow.},
  year = {2020},
  month = {sep},
  day = {7},
  language = {English},
  booktitle = {The Nineteenth Workshop on Constraint Modelling and Reformulation (ModRef 2020)},
}

@inproceedings{d868096c9b0a428f83a1d508f98736a5,
  title = {Towards portfolios of streamlined constraint models: a case study with the balanced academic curriculum problem},
  abstract = {Augmenting a base constraint model with additional constraints can strengthen the inferences made by a solver and therefore reduce search effort. We focus on the automatic addition of streamliner constraints, derived from the types present in an abstract Essence specification of a problem class of interest, which trade completeness for potentially very significant reduction in search. The refinement of streamlined Essence specifications into constraint models suitable for input to constraint solvers gives rise to a large number of modelling choices in addition to those required for the base Essence specification. Previous automated streamlining approaches have been limited in evaluating only a single default model for each streamlined specification. In this paper we explore the effect of model selection in the context of streamlined specifications. We propose a new best-first search method that generates a portfolio of Pareto Optimal streamliner-model combinations by evaluating for each streamliner a portfolio of models to search and explore the variability in performance and find the optimal model. Various forms of racing are utilised to constrain the computational cost of training. },
  keywords = {Constraint programming, Streamliners},
  author = {Patrick Spracklen and Nguyen Dang and {\"O}zg{\"u}r Akg{\"u}n and Ian Miguel},
  note = {Funding: This work is supported by EPSRC grant EP/P015638/1 and used the Cirrus UK National Tier-2 HPC Service at EPCC (http://www.cirrus.ac.uk) funded by the University of Edinburgh and EPSRC (EP/P020267/1). Nguyen Dang is a Leverhulme Early Career Fellow.; The 19th workshop on Constraint Modelling and Reformulation (ModRef), ModRef ; Conference date: 07-09-2020 Through 07-09-2020},
  year = {2020},
  month = {sep},
  day = {7},
  language = {English},
  booktitle = {The Nineteenth Workshop on Constraint Modelling and Reformulation (ModRef 2020)},
  url = {https://modref.github.io/ModRef2020.html},
}

@inproceedings{17528eb080e74cb6ac61a8387e9ac515,
  title = {Exploiting incomparability in solution dominance: improving general purpose constraint-based mining},
  abstract = {In data mining, finding interesting patterns is a challenging task. Constraint-based mining is a well-known approach to this, and one for which constraint programming has been shown to be a well-suited and generic framework. Constraint dominance programming (CDP) has been proposed as an extension that can capture an even wider class of constraint-based mining problems, by allowing us to compare relations between patterns. In this paper we improve CDP with the ability to specify an incomparability condition. This allows us to overcome two major shortcomings of CDP: finding dominated solutions that must then be filtered out after search, and unnecessarily adding dominance blocking constraints between incomparable solutions. We demonstrate the efficacy of our approach by extending the problem specification language ESSENCE and implementing it in a solver-independent manner on top of the constraint modelling tool CONJURE. Our experiments on pattern mining tasks with both a CP solver and a SAT solver show that using the incomparability condition during search significantly improves the efficiency of dominance programming and reduces (and often eliminates entirely) the need for post-processing to filter dominated solutions.},
  author = {Gokberk Kocak and Ozgur Akgun and Tias Guns and Miguel, {Ian James}},
  year = {2020},
  month = {aug},
  day = {29},
  doi = {10.3233/FAIA200110},
  language = {English},
  isbn = {9781643681009},
  series = {Frontiers in artificial intelligence and applications},
  publisher = {IOS Press},
  pages = {331--338},
  editor = {{De Giacomo}, Giuseppe and Alejandro Catala and Bistra Dilkina and Michela Milano and Sen{\'e}n Barro and Alberto Bugar{\'i}n and J{\'e}r{\^o}me Lang},
  booktitle = {The Twenty-Fourth European Conference on Artificial Intelligence (ECAI 2020)},
  address = {Netherlands},
  note = {The Twenty-Fourth European Conference on Artificial Intelligence (ECAI2020), ECAI2020 ; Conference date: 29-08-2020 Through 02-09-2020},
  url = {https://ecai2020.eu/},
}

@article{921447ff03114bdea8db07407ee51887,
  title = {Linking Scottish vital event records using family groups},
  abstract = {The reconstitution of populations through linkage of historical records is a powerful approach to generate longitudinal historical microdata resources of interest to researchers in various fields. Here we consider automated linking of the vital events recorded in the civil registers of birth, death and marriage compiled in Scotland, to bring together the various records associated with the demographic events in the life course of each individual in the population. From the histories, the genealogical structure of the population can then be built up. Rather than apply standard linkage techniques to link the individuals on the available certificates, we explore an alternative approach, inspired by the family reconstitution techniques adopted by historical demographers, in which the births of siblings are first linked to form family groups, after which intergenerational links between families can be established. We report a small-scale evaluation of this approach, using two district-level data sets from Scotland in the late nineteenth century, for which sibling links have already been created by demographers. We show that quality measures of up to 83% can be achieved on these data sets (using F-Measure, a combination of precision and recall). In the future, we intend to compare the results with a standard linkage approach and to investigate how these various methods may be used in a project which aims to link the entire Scottish population from 1856 to 1973.},
  keywords = {Scottish vital event records, Record linkage, Linkage methods, Group linkage, Population reconstruction, Digitising Scotland},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Alan Dearle and Kirby, {Graham Njal Cameron} and Eilidh Garrett and Dalton, {Thomas Stanley} and Peter Christen and Dibben, {Christopher John Lloyd} and Williamson, {Lee Emma Palmer}},
  note = {Funding: This work was supported by ESRC Grants ES/K00574X/2 “Digitising Scotland” and ES/L007487/1 “Administrative Data Research Centre – Scotland.”},
  year = {2020},
  month = {apr},
  day = {2},
  doi = {10.1080/01615440.2019.1571466},
  language = {English},
  volume = {53},
  pages = {130--146},
  journal = {Historical Methods: a Journal of Quantitative and Interdisciplinary History},
  issn = {0161-5440},
  publisher = {Routledge Taylor & Francis Group},
  number = {2},
}

@inproceedings{04b6f9716a204104b26a8a664d101d92,
  title = {Discriminating instance generation from abstract specifications: a case study with CP and MIP},
  abstract = {We extend automatic instance generation methods to allow cross-paradigm comparisons. We demonstrate that it is possible to completely automate the search for benchmark instances that help to discriminate between solvers. Our system starts from a high level human-provided problem specification, which is translated into a specification for valid instances. We use the automated algorithm configuration tool irace to search for instances, which are translated into inputs for both MIP and CP solvers by means of the Conjure, Savile Row, and MiniZinc tools. These instances are then solved by CPLEX and Chuffed, respectively. We constrain our search for instances by requiring them to exhibit a significant advantage for MIP over CP, or vice versa. Experimental results on four optimisation problem classes demonstrate the effectiveness of our method in identifying instances that highlight differences in performance of the two solvers.},
  keywords = {Constraint Programming, Instance generation, MIP},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Nguyen Dang and Ian Miguel and Salamon, {Andr{\'a}s Z.} and Patrick Spracklen and Christopher Stone},
  note = {This work is supported by EPSRC grant EP/P015638/1 and used the Cirrus UK National Tier-2 HPC Service at EPCC funded by the University of Edinburgh and EPSRC (EP/P020267/1).; 17th International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research, CPAIOR 2020 ; Conference date: 21-09-2020 Through 24-09-2020},
  year = {2020},
  doi = {10.1007/978-3-030-58942-4_3},
  language = {English},
  isbn = {9783030589417},
  series = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  pages = {41--51},
  editor = {Emmanuel Hebrard and Nysret Musliu},
  booktitle = {The Seventeenth International Conference of the Integration of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR 2020)},
  address = {Netherlands},
  url = {https://cpaior2020.dbai.tuwien.ac.at/},
}

@inproceedings{5d418525265e4c2189449a84ef9db61e,
  title = {Effective encodings of constraint programming models to SMT},
  abstract = {Satisfiability Modulo Theories (SMT) is a well-established methodology that generalises propositional satisfiability (SAT) by adding support for a variety of theories such as integer arithmetic and bit-vector operations. SMT solvers have made rapid progress in recent years. In part, the efficiency of modern SMT solvers derives from the use of specialised decision procedures for each theory. In this paper we explore how the Essence Prime constraint modelling language can be translated to the standard SMT-LIB language. We target four theories: bit-vectors (QF_BV), linear integer arithmetic (QF_LIA), non-linear integer arithmetic (QF_NIA), and integer difference logic (QF_IDL). The encodings are implemented in the constraint modelling tool Savile Row. In an extensive set of experiments, we compare our encodings for the four theories, showing some notable differences and complementary strengths. We also compare our new encodings to the existing work targeting SMT and SAT, and to a well-established learning CP solver. Our two proposed encodings targeting the theory of bit-vectors (QF_BV) both substantially outperform earlier work on encoding to QF_BV on a large and diverse set of problem classes.},
  keywords = {Constraint modelling, SMT, Automated reformulation},
  author = {Ewan Davidson and Ozgur Akgun and {Espasa Arxer}, Joan and Peter Nightingale},
  note = {Funding: UK EPSRC grant EP/P015638/1.; The Twenty-Sixth International Conference on Principles and Practice of Constraint Programming (CP 2020), CP 2020 ; Conference date: 07-09-2020 Through 11-09-2020},
  year = {2020},
  doi = {10.1007/978-3-030-58475-7_9},
  language = {English},
  isbn = {9783030584740},
  series = {Lecture Notes in Computer Science (Programming and Software Engineering)},
  publisher = {Springer},
  pages = {143--159},
  editor = {Helmut Simonis},
  booktitle = {The Twenty-Sixth International Conference on Principles and Practice of Constraint Programming (CP 2020)},
  address = {Netherlands},
  url = {https://cp2020.a4cp.org/},
}

@inproceedings{e74974988ec540bab669443ad10422b3,
  title = {Towards improving solution dominance with incomparability conditions: a case-study using Generator Itemset Mining},
  abstract = {Finding interesting patterns is a challenging task in data mining. Constraint based mining is a well-known approach to this, and one for which constraint programming has been shown to be a well-suited and generic framework.Dominance programming has been proposed as an extension that can capture aneven wider class of constraint-based mining problems, by allowing to comparerelations between patterns. In this paper, in addition to specifying a dominancerelation, we introduce the ability to specify an incomparability condition. Usingthese two concepts we devise a generic framework that can do a batch-wise searchthat avoids checking incomparable solutions. We extend the ESSENCE languageand underlying modelling pipeline to support this. We use generator itemset mining problem as a test case and give a declarative specification for that. We alsopresent preliminary experimental results on this specific problem class with a CPsolver backend to show that using the incomparability condition during searchcan improve the efficiency of dominance programming and reduces the need forpost-processing to filter dominated solutions.},
  keywords = {Constraint programming, Constraint modelling, Data mining, Itemset mining, Pattern mining, Dominance programming},
  author = {Gokberk Kocak and {\"O}zg{\"u}r Akg{\"u}n and Ian Miguel and Tias Guns},
  note = {Funding: EPSRC (EP/P015638/1).; The Twenty-Fifth International Conference on Principles and Practice of Constraint Programming (CP 2019), CP 2019 ; Conference date: 30-09-2019 Through 04-10-2019},
  year = {2019},
  month = {sep},
  day = {30},
  language = {English},
  booktitle = {The Eighteenth Workshop on Constraint Modelling and Reformulation (ModRef 2019)},
  url = {http://cp2019.a4cp.org},
}

@article{6d13ed10041c43c49530b2b55d688e91,
  title = {Solving computational problems in the theory of word-representable graphs},
  abstract = {A simple graph G = (V, E) is word-representable if there exists a word w over the alphabet V such that letters x and y alternate in w iff xy ∈ E. Word-representable graphs generalize several important classes of graphs. A graph is word-representable if it admits a semi-transitive orientation. We use semi-transitive orientations to enumerate connected non-word-representable graphs up to the size of 11 vertices, which led to a correction of a published result. Obtaining the enumeration results took 3 CPU years of computation.Also, a graph is word-representable if it is k-representable for some k, that is, if it can be represented using k copies of each letter. The minimum such k for a given graph is called graph's representation number. Our computational results in this paper not only include distribution of k-representable graphs on at most 9 vertices, but also have relevance to a known conjecture on these graphs. In particular, we find a new graph on 9 vertices with high representation number. Also, we prove that a certain graph has highest representation number among all comparability graphs on odd number of vertices.Finally, we introduce the notion of a k-semi-transitive orientation refining the notion of a semi-transitive orientation, and show computationally that the refinement is not equivalent to the original definition, unlike the equivalence of k-representability and word-representability. },
  keywords = {Word-representable graph, Representation number, Enumeration, Semi-transitive orientation, k-semi-transitive orientation},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Gent, {Ian P.} and Sergey Kitaev and Hans Zantema},
  year = {2019},
  month = {feb},
  day = {24},
  language = {English},
  volume = {22},
  journal = {Journal of Integer Sequences},
  issn = {1530-7638},
  publisher = {University of Waterloo},
  number = {2},
  url = {https://cs.uwaterloo.ca/journals/JIS/VOL22/Kitaev/kitaev11.html},
}

@article{4b54ed2e79924712bcf8e6fea3211d72,
  title = {How people visually represent discrete constraint problems},
  abstract = {Problems such as timetabling or personnel allocation can be modeled and solved using discrete constraint programming languages. However, while existing constraint solving software solves such problems quickly in many cases, these systems involve specialized languages that require significant time and effort to learn and apply. These languages are typically text-based and often difficult to interpret and understand quickly, especially for people without engineering or mathematics backgrounds. Visualization could provide an alternative way to model and understand such problems. Although many visual programming languages exist for procedural languages, visual encoding of problem specifications has not received much attention. Future problem visualization languages could represent problem elements and their constraints unambiguously, but without unnecessary cognitive burdens for those needing to translate their problem's mental representation into diagrams. As a first step towards such languages, we executed a study that catalogs how people represent constraint problems graphically. We studied three groups with different expertise: non-computer scientists, computer scientists and constraint programmers and analyzed their marks on paper (e.g., arrows), gestures (e.g., pointing) and the mappings to problem concepts (e.g., containers, sets). We provide foundations to guide future tool designs allowing people to effectively grasp, model and solve problems through visual representations.},
  keywords = {Problem visualization, Problem modeling, Problem solving, Constraint programming, Visual programming languages},
  author = {Xu Zhu and Miguel Nacenta and {\"O}zg{\"u}r Akg{\"u}n and Nightingale, {Peter William}},
  note = {Funding: This work is supported by EPSRC grants DTG1796157 and EP/P015638/1.},
  year = {2019},
  month = {jan},
  day = {24},
  doi = {10.1109/TVCG.2019.2895085},
  language = {English},
  volume = {26},
  pages = {2603 -- 2619},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  issn = {1077-2626},
  publisher = {IEEE Computer Society},
  number = {8},
}

@article{1a41bce3240a48d8a8c1b977f34215ea,
  title = {Cloud benchmarking for maximising performance of scientific applications},
  abstract = {How can applications be deployed on the cloud to achieve maximum performance? This question is challenging to address with the availability of a wide variety of cloud Virtual Machines (VMs) with different performance capabilities. The research reported in this paper addresses the above question by proposing a six step benchmarking methodology in which a user provides a set of weights that indicate how important memory, local communication, computation and storage related operations are to an application. The user can either provide a set of four abstract weights or eight fine grain weights based on the knowledge of the application. The weights along with benchmarking data collected from the cloud are used to generate a set of two rankings - one based only on the performance of the VMs and the other takes both performance and costs into account. The rankings are validated on three case study applications using two validation techniques. The case studies on a set of experimental VMs highlight that maximum performance can be achieved by the three top ranked VMs and maximum performance in a cost-effective manner is achieved by at least one of the top three ranked VMs produced by the methodology.},
  keywords = {Cloud benchmarking, Cloud performance, Benchmarking methodology, Cloud ranking},
  author = {Blesson Varghese and Ozgur Akgun and Miguel, {Ian James} and Thai, {Long Thanh} and Barker, {Adam David}},
  note = {This research was pursued under the EPSRC grant, EP/K015745/1, a Royal Society Industry Fellowship and an AWS Education Research grant.},
  year = {2019},
  month = {jan},
  day = {1},
  doi = {10.1109/TCC.2016.2603476},
  language = {English},
  volume = {7},
  pages = {170--182},
  journal = {IEEE Transactions on Cloud Computing},
  issn = {2168-7161},
  publisher = {IEEE},
  number = {1},
}

@inproceedings{ce8510eb25d34db48cd8c3c7682edfdf,
  title = {Automatic streamlining for constrained optimisation},
  abstract = {Augmenting a base constraint model with additional constraints can strengthen the inferences made by a solver and therefore reduce search effort. We focus on the automatic addition of streamliner constraints, which trade completeness for potentially very significant reduction in search. Recently an automated approach has been proposed, which produces streamliners via a set of streamliner generation rules. This existing automated approach to streamliner generation has two key limitations. First, it outputs a single streamlined model. Second, the approach is limited to satisfaction problems. We remove both limitations by providing a method to produce automatically a portfolio of streamliners, each representing a different balance between three criteria: how aggressively the search space is reduced, the proportion of training instances for which the streamliner admitted at least one solution, and the average reduction in quality of the objective value versus the unstreamlined model. In support of our new method, we present an automated approach to training and test instance generation, and provide several approaches to the selection and application of the streamliners from the portfolio. Empirical results demonstrate drastic improvements both to the time required to find good solutions early and to prove optimality on three problem classes.},
  keywords = {Constraint programming, Streamliners},
  author = {Patrick Spracklen and Nguyen Dang and Ozgur Akgun and Miguel, {Ian James}},
  note = {Funding: UK EPSRC grant EP/P015638/1.; The Twenty-Fifth International Conference on Principles and Practice of Constraint Programming (CP 2019), CP 2019 ; Conference date: 30-09-2019 Through 04-10-2019},
  year = {2019},
  doi = {10.1007/978-3-030-30048-7_22},
  language = {English},
  isbn = {9783030300470},
  series = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  pages = {366--383},
  editor = {Thomas Schiex and {de Givry}, Simon},
  booktitle = {The Twenty-Fifth International Conference on Principles and Practice of Constraint Programming (CP 2019)},
  address = {Netherlands},
  url = {http://cp2019.a4cp.org},
}

@inproceedings{b1d1e65f1ecf4431bc087cff9ef9b9dd,
  title = {Instance generation via generator instances},
  abstract = {Access to good benchmark instances is always desirable when developing new algorithms, new constraint models, or when comparing existing ones. Hand-written instances are of limited utility and are time-consuming to produce. A common method for generating instances is constructing special purpose programs for each class of problems. This can be better than manually producing instances, but developing such instance generators also has drawbacks. In this paper, we present a method for generating graded instances completely automatically starting from a class-level problem specification. A graded instance in our present setting is one which is neither too easy nor too difficult for a given solver. We start from an abstract problem specification written in the Essence language and provide a system to transform the problem specification, via automated type-specific rewriting rules, into a new abstract specification which we call a generator specification. The generator specification is itself parameterised by a number of integer parameters; these are used to characterise a certain region of the parameter space. The solutions of each such generator instance form valid problem instances. We use the parameter tuner irace to explore the space of possible generator parameters, aiming to find parameter values that yield graded instances. We perform an empirical evaluation of our system for five problem classes from CSPlib, demonstrating promising results.},
  keywords = {Automated modelling, Instance generation, Parameter tuning},
  author = {Ozgur Akgun and Nguyen Dang and Miguel, {Ian James} and Salamon, {Andr{\'a}s Z.} and Stone, {Christopher Luciano}},
  note = {Funding: UK EPSRC grant EP/P015638/1.; The Twenty-Fifth International Conference on Principles and Practice of Constraint Programming (CP 2019), CP 2019 ; Conference date: 30-09-2019 Through 04-10-2019},
  year = {2019},
  doi = {10.1007/978-3-030-30048-7_1},
  language = {English},
  isbn = {9783030300470},
  series = {Lecture Notes in Computer Science (Programming and Software Engineering)},
  publisher = {Springer},
  pages = {3--19},
  editor = {Thomas Schiex and {de Givry}, Simon},
  booktitle = {The Twenty-Fifth International Conference on Principles and Practice of Constraint Programming (CP 2019)},
  address = {Netherlands},
  url = {http://cp2019.a4cp.org},
}

@inproceedings{921a03b374654acdb3cf8b608e1ef86a,
  title = {Closed frequent itemset mining with arbitrary side constraints},
  abstract = {Frequent itemset mining (FIM) is a method for finding regularities in transaction databases. It has several application areas, such as market basket analysis, genome analysis, and drug design. Finding frequent itemsets allows further analysis to focus on a small subset of the data. For large datasets the number of frequent itemsets can also be very large, defeating their purpose. Therefore, several extensions to FIM have been studied, such as adding high-utility (or low-cost) constraints and only finding closed (or maximal) frequent itemsets. This paper presents a constraint programming based approach that combines arbitrary side constraints with closed frequent itemset mining. Our approach allows arbitrary side constraints to be expressed in a high level and declarative language which is then translated automatically for efficient solution by a SAT solver. We compare our approach with state-of-the-art algorithms via the MiningZinc system (where possible) and show significant contributions in terms of performance and applicability.},
  keywords = {Data mining, Pattern mining, Frequent itemset mining, Closed frequent itemset mining, Constraint modelling},
  author = {Gokberk Kocak and Ozgur Akgun and Miguel, {Ian James} and Nightingale, {Peter William}},
  year = {2018},
  month = {nov},
  day = {17},
  doi = {10.1109/ICDMW.2018.00175},
  language = {English},
  isbn = {9781538692899},
  pages = {1224 -- 1232},
  editor = {Hanghang Tong and Li, {Zhenhui (Jessie)} and Feida Zhu and Jeffrey Yu},
  booktitle = {IEEE International Conference on Data Mining Workshops (ICDMW 2018)},
  publisher = {IEEE Computer Society},
  address = {United States},
  note = {Workshop on Optimization Based Techniques for Emerging Data Mining Problems (OEDM 2018), OEDM 2018 ; Conference date: 17-11-2018 Through 20-11-2018},
  url = {https://qizhiquan.github.io/OEDM-18/},
}

@inproceedings{8f516aac022d4bcdb34e5f63976bdd78,
  title = {Memory consistency models using constraints},
  abstract = {Memory consistency models (MCMs) are at the heart of concurrent programming. They represent the behaviour of concurrent programs at the chip level. To test these models small program snippets called litmus test are generated, which show allowed or forbidden behaviour of different MCMs. This paper is showcasing the use of constraint programming to automate the generation and testing of litmus tests for memory consistency models. We produce a few exemplary case studies for two MCMs, namely Sequential Consistency and Total Store Order. These studies demonstrate the flexibility of constrains programming in this context and lay foundation to the direct verification of MCMs against the software facing cache coherence protocols. },
  keywords = {Memory consistency, Concurrent programming, Litmus tests, Constraints programming, Modelling},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Ruth Hoffmann and Susmit Sarkar},
  year = {2018},
  month = {aug},
  day = {27},
  language = {English},
  booktitle = {The Seventeenth Workshop on Constraint Modelling and Reformulation (ModRef 2018)},
  note = {The Twenty-Fourth International Conference on Principles and Practice of Constraint Programming (CP 2018), CP 2018 ; Conference date: 27-08-2018 Through 31-08-2018},
  url = {http://cp2018.a4cp.org/},
}

@inproceedings{56bd918cab3c4a66b6fae5b71f88b1b6,
  title = {Modelling Langford's Problem: a viewpoint for search},
  abstract = {The performance of enumerating all solutions to an instance of Langford's Problem is sensitive to the model and the search strategy. In this paper we compare the performance of a large variety of models, all derived from two base viewpoints. We empirically show that a channelled model with a static branching order on one of the viewpoints offers the best performance out of all the options we consider. Surprisingly, one of the base models proves very effective for propagation, while the other provides an effective means of stating a static search order. },
  author = {{\"O}zg{\"u}r Akg{\"u}n and Ian Miguel},
  note = {Funding: EPSRC (EP/P015638/1).; 24th International Conference on Principles and Practice of Constraint Programming (CP 2018), CP 2018 ; Conference date: 27-08-2018 Through 31-08-2018},
  year = {2018},
  month = {aug},
  day = {27},
  language = {English},
  booktitle = {The Seventeenth Workshop on Constraint Modelling and Reformulation (ModRef 2018)},
  url = {http://cp2018.a4cp.org/},
}

@inproceedings{413b9d1324cf4826b5ea1a130eb96159,
  title = {A framework for constraint based local search using ESSENCE},
  abstract = {Structured Neighbourhood Search (SNS) is a framework for constraint-based local search for problems expressed in the Essence abstract constraint specification language. The local search explores a structured neighbourhood, where each state in the neighbourhood preserves a high level structural feature of the problem. SNS derives highly structured problem-specific neighbourhoods automatically and directly from the features of the ESSENCE specification of the problem. Hence, neighbourhoods can represent important structural features of the problem, such as partitions of sets, even if that structure is obscured in the low-level input format required by a constraint solver. SNS expresses each neighbourhood as a constrained optimisation problem, which is solved with a constraint solver. We have implemented SNS, together with automatic generation of neighbourhoods for high level structures, and report high quality results for several optimisation problems.},
  keywords = {Constraints and SAT: constraint satisfaction, Constraints and SAT: modeling; formulation, Constraints and SAT: constraint ptimisation, Constraints and SAT: Constraints: solvers and tools},
  author = {Ozgur Akgun and Attieh, {Saad Wasim A} and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William} and Salamon, {Andr{\'a}s Z.} and Patrick Spracklen and Wetter, {James Patrick}},
  note = {Funding: UK Engineering & Physical Sciences Research Council (EPSRC) grants EP/P015638/1and EP/P026842/1.; 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence, IJCAI-ECAI-18 ; Conference date: 13-07-2018 Through 19-07-2018},
  year = {2018},
  month = {jul},
  day = {13},
  doi = {10.24963/ijcai.2018/173},
  language = {English},
  pages = {1242--1248},
  editor = {J{\'e}r{\^o}me Lang},
  booktitle = {The Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI 2018)},
  publisher = {International Joint Conferences on Artificial Intelligence},
  url = {https://www.ijcai-18.org/},
}

@conference{c4f1f1048f82470c9f62be594dd5a3a3,
  title = {Validating Synthetic Longitudinal Populations for evaluation of Population Data Linkage},
  abstract = {Background{\textquoteright}Gold-standard{\textquoteright} data to evaluate linkage algorithms are rare. Synthetic data have the advantage that all the true links are known. In the domain of population reconstruction, the ability to synthesize populations on demand, with varying characteristics, allows a linkage approach to be evaluated across a wide range of data. We have implemented ValiPop, a microsimulation model, for this purpose.ApproachValiPop can create many varied populations based upon sets of desired population statistics, thus allowing linkage algorithms to be evaluated across many populations, rather than across a limited number of real world {\textquoteright}gold-standard{\textquoteright} data sets.Given the potential interactions between different desired population statistics, the creation of a population does not necessarily imply that all desired population statistics have been met. To address this we have developed a statistical approach to validate the adherence of created populations to the desired statistics, using a generalized linear model.This talk will discuss the benefits of synthetic data for data linkage evaluation, the approach to validating created populations, and present the results of some initial linkage experiments using our synthetic data.},
  author = {Dalton, {Thomas Stanley} and Kirby, {Graham Njal Cameron} and Alan Dearle and Ozgur Akgun and MacKenzie, {Monique Lea}},
  year = {2018},
  month = {jun},
  day = {11},
  doi = {10.23889/ijpds.v3i2.504},
  language = {English},
}

@inproceedings{b170a1edfb03483a860a1e1482829944,
  title = {Automatic discovery and exploitation of promising subproblems for tabulation},
  abstract = {The performance of a constraint model can often be improved by converting a subproblem into a single table constraint. In this paper we study heuristics for identifying promising subproblems. We propose a small set of heuristics to identify common cases such as expressions that will propagate weakly. The process of discovering promising subproblems and tabulating them is entirely automated in the tool Savile Row. A cache is implemented to avoid tabulating equivalent subproblems many times. We give a simple algorithm to generate table constraints directly from a constraint expression in Savile Row. We demonstrate good performance on the benchmark problems used in earlier work on tabulation, and also for several new problem classes. },
  author = {Ozgur Akgun and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William} and Salamon, {Andr{\'a}s Z.}},
  note = {Funding: EP/P015638/1 and EP/P026842/1. Dr Jefferson holds a Royal Society University Research Fellowship.; 24th International Conference on Principles and Practice of Constraint Programming (CP 2018), CP 2018 ; Conference date: 27-08-2018 Through 31-08-2018},
  year = {2018},
  doi = {10.1007/978-3-319-98334-9_1},
  language = {English},
  isbn = {9783319983332},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  pages = {3--12},
  editor = {John Hooker},
  booktitle = {The Twenty-Fourth International Conference on Principles and Practice of Constraint Programming (CP 2018)},
  address = {Netherlands},
  url = {http://cp2018.a4cp.org/},
}

@inproceedings{6eef5285c1a0471ebba55a9179298de8,
  title = {Automatic generation and selection of streamlined constraint models via Monte Carlo search on a model lattice},
  abstract = {Streamlined constraint reasoning is the addition of uninferred constraints to a constraint model to reduce the search space, while retaining at least one solution. Previously it has been established that it is possible to generate streamliners automatically from abstract constraint specifications in Essence and that effective combinations of streamliners can allow instances of much larger scale to be solved. A shortcoming of the previous approach was the crude exploration of the power set of all combinations using depth and breadth first search. We present a new approach based on Monte Carlo search over the lattice of streamlined models, which efficiently identifies effective streamliner combinations.},
  author = {Patrick Spracklen and Ozgur Akgun and Miguel, {Ian James}},
  note = {Funding: EPSRC EP/P015638/1.},
  year = {2018},
  doi = {10.1007/978-3-319-98334-9_24},
  language = {English},
  isbn = {9783319983332},
  series = {Lecture Notes in Computer Science (including subseries Programming and Software Engineering)},
  publisher = {Springer},
  pages = {362--372},
  editor = {John Hooker},
  booktitle = {The Twenty-Fourth International Conference on Principles and Practice of Constraint Programming (CP 2018)},
  address = {Netherlands},
}

@inproceedings{53282bbae2084f9ab816728d0a72fd7b,
  title = {Metamorphic testing of constraint solvers},
  abstract = {Constraint solvers are complex pieces of software and are notoriously difficult to debug. In large part this is due to the difficulty of pinpointing the source of an error in the vast searches these solvers perform, since the effect of an error may only come to light long after the error is made. In addition, an error does not necessarily lead to the wrong result, further complicating the debugging process. A major source of errors in a constraint solver is the complex constraint propagation algorithms that provide the inference that controls and directs the search. In this paper we show that metamorphic testing is a principled way to test constraint solvers by comparing two different implementations of the same constraint. Specifically, specialised propagators for the constraint are tested against the general purpose table constraint propagator. We report on metamorphic testing of the constraint solver Minion. We demonstrate that the metamorphic testing method is very effective for finding artificial bugs introduced by random code mutation.},
  author = {Ozgur Akgun and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William}},
  note = {Funding: EPSRC EP/P015638/1 and EP/P026842/1. Dr Jefferson holds a Royal Society University Research Fellowship.; 24th International Conference on Principles and Practice of Constraint Programming (CP 2018), CP 2018 ; Conference date: 27-08-2018 Through 31-08-2018},
  year = {2018},
  doi = {10.1007/978-3-319-98334-9_46},
  language = {English},
  isbn = {9783319983332},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  pages = {727--736},
  editor = {John Hooker},
  booktitle = {The Twenty-Fourth International Conference on Principles and Practice of Constraint Programming (CP 2018)},
  address = {Netherlands},
  url = {http://cp2018.a4cp.org/},
}

@inproceedings{2bf3e6cc02c540439b29015862135919,
  title = {Using metric space indexing for complete and efficient record linkage},
  abstract = {Record linkage is the process of identifying records that refer to the same real-world entities in situations where entity identifiers are unavailable. Records are linked on the basis of similarity between common attributes, with every pair being classified as a link or non-link depending on their similarity. Linkage is usually performed in a three-step process: first, groups of similar candidate records are identified using indexing, then pairs within the same group are compared in more detail, and finally classified. Even state-of-the-art indexing techniques, such as locality sensitive hashing, have potential drawbacks. They may fail to group together some true matching records with high similarity, or they may group records with low similarity, leading to high computational overhead. We propose using metric space indexing (MSI) to perform complete linkage, resulting in a parameter-free process combining indexing, comparison and classification into a single step delivering complete and efficient record linkage. An evaluation on real-world data from several domains shows that linkage using MSI can yield better quality than current indexing techniques, with similar execution cost, without the need for domain knowledge or trial and error to configure the process.},
  keywords = {Entity resolution, Data matching, Similarity search, Blocking},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Alan Dearle and Kirby, {Graham Njal Cameron} and Peter Christen},
  year = {2018},
  doi = {10.1007/978-3-319-93040-4_8},
  language = {English},
  isbn = {9783319930398},
  series = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  pages = {89--101},
  editor = {Dinh Phung and Tseng, {Vincent S.} and Geoff Webb and Bao Ho and Mohadeseh Ganji and Lida Rashidi},
  booktitle = {The Twenty-Second Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2018)},
  address = {Netherlands},
  note = {22nd Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2018), PAKDD 2018 ; Conference date: 03-06-2018 Through 06-06-2018},
  url = {http://prada-research.net/pakdd18/},
}

@article{8a56ff34e5bc4dada3bcc63d391de55e,
  title = {Automatically improving constraint models in Savile Row},
  abstract = {When solving a combinatorial problem using Constraint Programming (CP) or Satisfiability (SAT), modelling and formulation are vital and difficult tasks. Even an expert human may explore many alternatives in modelling a single problem. We make a number of contributions in the automated modelling and reformulation of constraint models. We study a range of automated reformulation techniques, finding combinations of techniques which perform particularly well together. We introduce and describe in detail a new algorithm, X-CSE, to perform Associative-Commutative Common Subexpression Elimination (AC-CSE) in constraint problems, significantly improving existing CSE techniques for associative and commutative operators such as +. We demonstrate that these reformulation techniques can be integrated in a single automated constraint modelling tool, called Savile Row, whose architecture we describe. We use Savile Row as an experimental testbed to evaluate each reformulation on a set of 50 problem classes, with 596 instances in total. Our recommended reformulations are well worthwhile even including overheads, especially on harder instances where solver time dominates. With a SAT solver we observed a geometric mean of 2.15 times speedup compared to a straightforward tailored model without recommended reformulations. Using a CP solver, we obtained a geometric mean of 5.96 times speedup for instances taking over 10 seconds to solve.},
  keywords = {Constraint satisfaction, Common subexpression elimination, Modelling, Reformulation, Propositional satisfiability},
  author = {Peter Nightingale and {\"O}zg{\"u}r Akg{\"u}n and Gent, {Ian P.} and Christopher Jefferson and Ian Miguel and Patrick Spracklen},
  note = {Authors thank the EPSRC for funding this work through grants EP/H004092/1, EP/K015745/1, EP/M003728/1, and EP/P015638/1. In addition, Dr Jefferson is funded by a Royal Society University Research Fellowship.},
  year = {2017},
  month = {oct},
  doi = {10.1016/j.artint.2017.07.001},
  language = {English},
  volume = {251},
  pages = {35--61},
  journal = {Artificial Intelligence},
  issn = {0004-3702},
  publisher = {Elsevier},
}

@conference{1b2ed603b33f4709870e2b38480f829d,
  title = {Learning From Past Links: Understanding the Limits of Linkage Quality},
  abstract = {The Digitising Scotland project aims to link 25 million vital event records from 1850s to 1970s. We aim to develop automatic approaches to probabilistic, similarity based record linkage. Linkage quality depends on the choices of keys and similarity measures. However, until now the effect of these choices has been unclear. We study the theoretical limits of automated linkage by performing a post-linkage analysis on two datasets, one from the Isle of Skye and one from Kilmarnock, previously linked by historical demographers. In these datasets, individuals appear on multiple certificates. The linkage problem involves unifying these occurrences e.g. between births and deaths, known as Entity Resolution. This requires the choice of particular keys, a similarity measure and a threshold signalling equivalence. We calculate linkage quality metrics–precision, recall, and F-measure–for 4 different key combinations, different similarity measures, and a range of threshold values. We present the distribution of similarity values for links and non-links for each configuration and data-set. From these results, we hope to understand the limits of automated probabilistic record linkage. We will use this understanding to inform our approach to the linkage of new unlinked datasets such as the Digitising Scotland dataset. We would welcome the opportunity to apply this approach to other linked demographic datasets.},
  author = {Ozgur Akgun and Alan Dearle and Eilidh Garrett and Kirby, {Graham Njal Cameron}},
  year = {2017},
  month = {sep},
  day = {6},
  language = {English},
  note = {British Society for Population Studies Annual Conference 2017, BSPS Annual Conference ; Conference date: 06-09-2017 Through 08-09-2017},
  url = {http://www.lse.ac.uk/socialPolicy/Researchcentresandgroups/BSPS/annualConference/Home.aspx},
}

@conference{b23bf2749f964504a25b98b76298e524,
  title = {Evaluating record linkage: creating longitudinal synthetic data to provide gold-standard linked data sets},
  abstract = {{\textquoteleft}Gold-standard{\textquoteright} data to evaluate linkage algorithms are rare. Synthetic data have the advantage that all the true links are known. In the domain of population reconstruction, the ability to synthesise populations on demand, with varying characteristics, allows a linkage approach to be evaluated across a wide range of data sets.We present a micro-simulation model for generating such synthetic populations, taking as input a set of desired statistical properties. It then outlines how these desired properties are verified in the generated populations, and the intended approach to using generated populations to evaluate linkage algorithms. We envisage a sequence of experiments where a set of populations are generated to consider how linkage quality varies across different populations: with the same characteristics, with differing characteristics, and with differing types and levels of corruption. The performance of an approach at scale is also considered.},
  keywords = {record linkage},
  author = {Dalton, {Thomas Stanley} and Alan Dearle and Kirby, {Graham Njal Cameron} and Ozgur Akgun},
  year = {2017},
  month = {may},
  day = {11},
  language = {English},
  note = {Workshop for the Systematic Linking of Historical Records ; Conference date: 11-05-2017 Through 13-05-2017},
  url = {http://recordlink.org},
}

@conference{0f94ac46dc094dbfbf3e719ec7b8e43c,
  title = {Probabilistic linkage of vital event records in Scotland using familial groups},
  abstract = {We report on the assembly of longitudinal data from Scottish birth, death and marriage records representing eighteen million individuals. An experimental approach based on familial groups starts by gathering parents and their siblings into bundles with the aim of (as near of possible) partitioning the certificates into familial groups. This may be achieved by bundling marriage and birth certificates according to a signature derived from their attributes. This is similar to but different from blocking used in most entity resolution schemes where certificates of one kind are gathered together. We have experimented with these techniques using hand coded data from an historic Scottish dataset as a gold standard for comparison. In this paper we will report on our techniques and some preliminary results from our experiments.},
  keywords = {record linkage},
  author = {Ozgur Akgun and Dalton, {Thomas Stanley} and Alan Dearle and Eilidh Garrett and Kirby, {Graham Njal Cameron}},
  year = {2017},
  month = {may},
  day = {11},
  language = {English},
  note = {Workshop for the Systematic Linking of Historical Records ; Conference date: 11-05-2017 Through 13-05-2017},
  url = {http://recordlink.org},
}

@conference{421d8687dc4e4d64b23a7b8eeb63f63b,
  title = {An identifier scheme for the Digitising Scotland project},
  abstract = {The Digitising Scotland project is having the vital records of Scotland transcribed from images of the original handwritten civil registers . Linking the resulting dataset of 24 million vital records covering the lives of 18 million people is a major challenge requiring improved record linkage techniques. Discussions within the multidisciplinary, widely distributed Digitising Scotland project team have been hampered by the teams in each of the institutions using their own identification scheme. To enable fruitful discussions within the Digitising Scotland team, we required a mechanism for uniquely identifying each individual represented on the certificates. From the identifier it should be possible to determine the type of certificate and the role each person played. We have devised a protocol to generate for any individual on the certificate a unique identifier, without using a computer, by exploiting the National Records of Scotland•{\`A}_s registration districts. Importantly, the approach does not rely on the handwritten content of the certificates which reduces the risk of the content being misread resulting in an incorrect identifier. The resulting identifier scheme has improved the internal discussions within the project. This paper discusses the rationale behind the chosen identifier scheme, and presents the format of the different identifiers. The work reported in the paper was supported by the British ESRC under grants ES/K00574X/1(Digitising Scotland) and ES/L007487/1 (Administrative Data Research Center - Scotland).},
  keywords = {record linkage},
  author = {Ozgur Akgun and Ahmad Al-Sidiqi and Peter Christen and Dalton, {Thomas Stanley} and Alan Dearle and Dibben, {Christopher John Lloyd} and Eilidh Garrett and Alasdair Gray and Kirby, {Graham Njal Cameron} and Alice Reid},
  year = {2017},
  month = {apr},
  day = {2},
  language = {English},
  note = {UK Administrative Data Research Network Annual Research Conference : Social science using administrative data for public benefit, ADRN2017 ; Conference date: 01-06-2017 Through 02-06-2017},
  url = {http://www.adrn2017.net},
}

@conference{2c922d20ebd541c7a1673a55e379b693,
  title = {Evaluating population data linkage: assessing stability, scalability, resilience and robustness across many data sets for comprehensive linkage evaluation},
  abstract = {Data linkage approaches are often evaluated with small or few data sets. If a linkage approach is to be used widely, quantifying its performance with varying data sets would be beneficial. In addition, given a data set needs to be linked, the true links are by definition unknown. The success of a linkage approach is thus difficult to comprehensively evaluate. This talk focuses on the use of many synthetic data sets for the evaluation of linkage quality achieved by automatic linkage algorithms in the domain of population reconstruction. It presents an evaluation approach which considers linkage quality when characteristics of the population are varied. We envisage a sequence of experiments where a set of populations are generated to consider how linkage quality varies across different populations: with the same characteristics, with differing characteristics, and with differing types and levels of corruption. The performance of an approach at scale is also considered. The approach to generate synthetic populations with varying characteristics on demand will also be addressed. The use of synthetic populations has the advantage that all the true links are known, thus allowing evaluation as if with real-world 'gold-standard' linked data sets. Given the large number of data sets evaluated against we also give consideration as to how to present these findings. The ability to assess variations in linkage quality across many data sets will assist in the development of new linkage approaches and identifying areas where existing linkage approaches may be more widely applied.},
  keywords = {data linkage},
  author = {Dalton, {Thomas Stanley} and Ozgur Akgun and Ahmad Al-Sediqi and Peter Christen and Alan Dearle and Eilidh Garrett and Alasdair Gray and Kirby, {Graham Njal Cameron} and Alice Reid},
  year = {2017},
  month = {apr},
  day = {2},
  language = {English},
  note = {UK Administrative Data Research Network Annual Research Conference : Social science using administrative data for public benefit, ADRN2017 ; Conference date: 01-06-2017 Through 02-06-2017},
  url = {http://www.adrn2017.net},
}

@conference{5caabedd32764dbbbe8c81724c8cd9e2,
  title = {Record linking using metric space similarity search},
  abstract = {Record linking often employs blocking to reduce the computational complexity of full pairwise comparison. A key is formed from a subset of record attributes. Those records with the same key values are blocked together for detailed comparison. Use of a single blocking key fails to detect many true matches if records contain missing values or errors, since only those records with the same key values are compared. To address missing values, it is common to repeat the matching process using multiple blocking keys, to match records that are identical in a subset of the fields. The presence of erroneous values may be addressed by blocking using key values mapped to a canonical form (e.g. Soundex). However, this does not address other problems such as single digit transcription errors in dates.Blocking is used to categorise records that are candidate matches, in preparation for a pairwise comparison phase which may use various distance metrics, depending on the domain of the values being compared. Each blocking process defines a partition of records. The comparison operations are only applied to pairs of records within the same category.In some contexts, it may be useful to have flexible control over the precision/recall trade-off, depending on the intended use for the matched data, and the degree of conservatism required of the identified links. With blocking, this flexibility is limited by the number of sensible blocking keys that can be identified.In this talk, we describe experiments with a technique based on similarity searching over metric spaces, which appears to offer greater flexibility, and describe some preliminary results using an historic Scottish dataset. },
  keywords = {record linkage},
  author = {Alan Dearle and Kirby, {Graham Njal Cameron} and Ozgur Akgun and Dalton, {Thomas Stanley}},
  year = {2017},
  month = {apr},
  day = {2},
  language = {English},
  note = {UK Administrative Data Research Network Annual Research Conference : Social science using administrative data for public benefit, ADRN2017 ; Conference date: 01-06-2017 Through 02-06-2017},
  url = {http://www.adrn2017.net},
}

@inproceedings{03feab994c4544a49e1387c269b809e0,
  title = {Exploiting short supports for improved encoding of arbitrary constraints into SAT},
  abstract = {Encoding to SAT and applying a highly efficient modern SAT solver is an increasingly popular method of solving finite-domain constraint problems. In this paper we study encodings of arbitrary constraints where unit propagation on the encoding provides strong reasoning. Specifically, unit propagation on the encoding simulates generalised arc consistency on the original constraint. To create compact and efficient encodings we use the concept of short support. Short support has been successfully applied to create efficient propagation algorithms for arbitrary constraints. A short support of a constraint is similar to a satisfying tuple however a short support is not required to assign every variable in scope. Some variables are left free to take any value. In some cases a short support representation is smaller than the table of satisfying tuples by an exponential factor. We present two encodings based on short supports and evaluate them on a set of benchmark problems, demonstrating a substantial improvement over the state of the art. },
  author = {{\"O}zg{\"u}r Akg{\"u}n and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William}},
  year = {2016},
  doi = {10.1007/978-3-319-44953-1_1},
  language = {English},
  isbn = {9783319449524},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  pages = {3--12},
  editor = {Michael Rueher},
  booktitle = {The Twenty-Second International Conference on Principles and Practice of Constraint Programming (CP 2016)},
  address = {Netherlands},
  note = {The Twenty-Second International Conference on Principles and Practice of Constraint Programming (CP 2016) ; Conference date: 05-09-2016 Through 09-09-2016},
}

@inproceedings{3881455b36054960a6bae0910108ab95,
  title = {Cloud-based e-Infrastructure for scheduling astronomical observations},
  abstract = {Gravitational microlensing exploits a transient phenomenon where an observed star is brightened due to deflection of its light by the gravity of an intervening foreground star. It is conjectured that this technique can be used to measurethe abundance of planets throughout the Milky Way. In order to undertake efficient gravitational microlensing an observation schedule must be constructed such that various targets are observed while undergoing a microlensing event. In this paper, we propose a cloud-based e-Infrastructure that currently supportsfour methods to compute candidate schedules via the application of local search and probabilistic meta-heuristics. We then validate the feasibility of the e-Infrastructure by evaluating the methods on historic data. The experiments demonstrate that the use of on-demand cloud resources for the e-Infrastructure can allow better schedules to be found more rapidly.},
  author = {Wetter, {James Patrick} and Ozgur Akgun and Barker, {Adam David} and Martin Dominik and Miguel, {Ian James} and Blesson Varghese},
  note = {This research was pursued under the EPSRC grant {\textquoteleft}Working Together: Constraint Programming and Cloud Computing{\textquoteright} (EP/K015745/1) and an Amazon Web Services (AWS) Education Research Grant. ; 11th IEEE International Conference on eScience ; Conference date: 31-08-2015 Through 04-09-2015},
  year = {2015},
  month = {aug},
  day = {31},
  doi = {10.1109/eScience.2015.54},
  language = {English},
  pages = {362--370},
  booktitle = {The Eleventh IEEE International Conference on e-Science (e-Science 2015)},
  publisher = {IEEE Computer Society},
  address = {United States},
  url = {http://escience2015.mnm-team.org/},
}

@inproceedings{9dd26cea0c54476f8d0418df430909da,
  title = {Automatically generating streamlined constraint models with ESSENCE and CONJURE},
  abstract = {Streamlined constraint reasoning is the addition of uninferred constraints to a constraint model to reduce the search space, while retaining at least one solution. Previously, effective streamlined models have been constructed by hand, requiring an expert to examine closely solutions to small instances of a problem class and identify regularities. We present a system that automatically generates many conjectured regularities for a given Essence specification of a problem class by examining the domains of decision variables present in the problem specification. These conjectures are evaluated independently and in conjunction with one another on a set of instances from the specified class via an automated modelling tool-chain comprising of Conjure, Savile Row and Minion. Once the system has identified effective conjectures they are used to generate streamlined models that allow instances of much larger scale to be solved. Our results demonstrate good models can be identified for problems in combinatorial design, Ramsey theory, graph theory and group theory - often resulting in order of magnitude speed-ups.},
  author = {James Wetter and Ozgur Akgun and Ian Miguel},
  year = {2015},
  month = {aug},
  day = {13},
  doi = {10.1007/978-3-319-23219-5_34},
  language = {English},
  isbn = {9783319232188},
  series = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  pages = {480--496},
  editor = {Gilles Pesant},
  booktitle = {The Twenty-First International Conference on Principles and Practice of Constraint Programming (CP 2015)},
  address = {Netherlands},
  note = {The Twenty-First International Conference on the Principles and Practice of Constraint Programming, CP 2015 ; Conference date: 31-08-2015 Through 04-09-2015},
}

@inproceedings{0e8ec1d427b148acb1f0d20b9ef51659,
  title = {Cloud benchmarking for performance},
  abstract = {How can applications be deployed on the cloud to achieve maximum performance? This question has become significant and challenging with the availability of a wide variety of Virtual Machines (VMs) with different performance capabilities in the cloud. The above question is addressed by proposing a six step benchmarking methodology in which a user provides a set of four weights that indicate how important each of the following groups: memory, processor, computation and storage are to the application that needs to be executed on the cloud. The weights along with cloud benchmarking data are used to generate a ranking of VMs that can maximise performance of the application. The rankings are validated through an empirical analysis using two case study applications, the first is a financial risk application and the second is a molecular dynamics simulation, which are both representative of workloads that can benefit from execution on the cloud. Both case studies validate the feasibility of the methodology and highlight that maximum performance can be achieved on the cloud by selecting the top ranked VMs produced by the methodology.},
  author = {Blesson Varghese and Ozgur Akgun and Ian Miguel and Long Thai and Adam Barker},
  year = {2014},
  month = {dec},
  day = {15},
  doi = {10.1109/CloudCom.2014.28},
  language = {English},
  isbn = {9781479940936},
  pages = {535--540},
  booktitle = {The Sixth IEEE International Conference on Cloud Computing Technology and Science (CloudCom 2014)},
  publisher = {IEEE},
  note = {6th IEEE International Conference on Cloud Computing Technology and Science (CloudCom 2014), CloudCom 2014 ; Conference date: 15-12-2014 Through 18-12-2014},
}

@inproceedings{09b16cf13ed94fb1b2fed6296b5a75d1,
  title = {Optimal deployment of geographically distributed workflow engines on the Cloud},
  abstract = {When orchestrating Web service workflows, the geographical placement of the orchestration engine(s) can greatly affect workflow performance. Data may have to be transferred across long geographical distances, which in turn increases execution time and degrades the overall performance of a workflow. In this paper, we present a framework that, given a DAG-based workflow specification, computes the op- timal Amazon EC2 cloud regions to deploy the orchestration engines and execute a workflow. The framework incorporates a constraint model that solves the workflow deployment problem, which is generated using an automated constraint modelling system. The feasibility of the framework is evaluated by executing different sample workflows representative of sci- entific workloads. The experimental results indicate that the framework reduces the workflow execution time and provides a speed up of 1.3x-2.5x over centralised approaches.},
  keywords = {Workflow engine, Optimal deployment, Cloud computing, Workflow execution},
  author = {Long Thai and Adam Barker and Blesson Varghese and Ozgur Akgun and Ian Miguel},
  note = {This research was pursued under the EPSRC {\textquoteleft}Working Together: Constraint Programming and Cloud Computing{\textquoteright} grant, a Royal Society Industry Fellowship {\textquoteleft}Bringing Science to the Cloud{\textquoteright}, and an Amazon Web Services Education Research Grant. Date of Acceptance: 02/09/2014},
  year = {2014},
  month = {oct},
  day = {30},
  doi = {10.1109/CloudCom.2014.30},
  language = {English},
  isbn = {9781479940936 },
  pages = {811--816},
  booktitle = {The Sixth IEEE International Conference on Cloud Computing Technology and Science (CloudCom 2014)},
  publisher = {IEEE},
}

@inproceedings{4d61e32b389247fe849054856b224aa1,
  title = {Automatically improving constraint models in Savile Row through associative-commutative common subexpression elimination},
  abstract = {When solving a problem using constraint programming, constraint modelling is widely acknowledged as an important and difficult task. Even a constraint modelling expert may explore many models and spend considerable time modelling a single problem. Therefore any automated assistance in the area of constraint modelling is valuable. Common sub-expression elimination (CSE) is a type of constraint reformulation that has proved to be useful on a range of problems. In this paper we demonstrate the value of an extension of CSE called Associative-Commutative CSE (AC-CSE). This technique exploits the properties of associativity and commutativity of binary operators, for example in sum constraints. We present a new algorithm, X-CSE, that is able to choose from a larger palette of common subexpressions than previous approaches. We demonstrate substantial gains in performance using X-CSE. For example on BIBD we observed speed increases of more than 20 times compared to a standard model and that using X-CSE outperforms a sophisticated model from the literature. For Killer Sudoku we found that X-CSE can render some apparently difficult instances almost trivial to solve, and we observe speed increases up to 350 times. For BIBD and Killer Sudoku the common subexpressions are not present in the initial model: an important part of our methodology is reformulations at the preprocessing stage, to create the common subexpressions for X-CSE to exploit. In summary we show that X-CSE, combined with preprocessing and other reformulations, is a powerful technique for automated modelling of problems containing associative and commutative constraints.},
  keywords = {Symmetry, Breaking, System},
  author = {Peter Nightingale and Ozgur Akgun and Gent, {Ian P.} and Christopher Jefferson and Ian Miguel},
  note = {We would like to thank the Royal Society for funding through Dr Jefferson{\textquoteright}s URF, and the EPSRC for funding this work through grant EP/H004092/1.; 20th International Conference on the Principles and Practice of Constraint Programming (CP 2014), CP 2014 ; Conference date: 08-09-2014 Through 12-09-2014},
  year = {2014},
  month = {sep},
  day = {8},
  doi = {10.1007/978-3-319-10428-7_43},
  language = {English},
  isbn = {9783319104270},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  pages = {590--605},
  editor = {Barry O'Sullivan},
  booktitle = {The Twentieth International Conference on Principles and Practice of Constraint Programming (CP 2014)},
  address = {Netherlands},
}

@inbook{dd9347655a2d45f2bef81ebcb8778daa,
  title = {Breaking conditional symmetry in automated constraint modelling with CONJURE},
  abstract = {Many constraint problems contain symmetry, which can lead to redundant search. If a partial assignment is shown to be invalid, we are wasting time if we ever consider a symmetric equivalent of it. A particularly important class of symmetries are those introduced by the constraint modelling process: model symmetries. We present a systematic method by which the automated constraint modelling tool CONJURE can break conditional symmetry as it enters a model during refinement. Our method extends, and is compatible with, our previous work on automated symmetry breaking in CONJURE. The result is the automatic and complete removal of model symmetries for the entire problem class represented by the input specification. This applies to arbitrarily nested conditional symmetries and represents a significant step forward for automated constraint modelling.},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Ian Gent and Chris Jefferson and Ian Miguel and Peter Nightingale},
  note = {This work was supported by UK EPSRC EP/K015745/1. Jefferson is supported by a Royal Society University Research Fellowship.},
  year = {2014},
  month = {aug},
  day = {18},
  doi = {10.3233/978-1-61499-419-0-3},
  language = {English},
  isbn = {9781614994183},
  series = {Frontiers in Artificial Intelligence and Applications},
  publisher = {IOS Press},
  pages = {3--8},
  editor = {Torsten Schaub and Gerhard Friedrich and Barry O'Sullivan},
  booktitle = {The Twenty-First European Conference on Artificial Intelligence (ECAI 2014)},
  address = {Netherlands},
}

@misc{0ef98e79e0bd426eb92227f281f4ee4e,
  title = {Extensible automated constraint modelling via refinement of abstract problem specifications},
  abstract = {Constraint Programming (CP) is a powerful technique for solving large-scale combinatorial (optimisation) problems. Constraint solving a given problem proceeds in two phases: modelling and solving. Effective modelling has an huge impact on the performance of the solving process. This thesis presents a framework in which the users are not required to make modelling decisions, concrete CP models are automatically generated from a high level problem specification. In this framework, modelling decisions are encoded as generic rewrite rules applicable to many different problems. First, modelling decisions are divided into two broad categories. This categorisation guides the automation of each kind of modelling decision and also leads us to the architecture of the automated modelling tool. Second, a domain-specific declarative rewrite rule language is introduced. Thanks to the rule language, automated modelling transformations and the core system are decoupled. The rule language greatly increases the extensibility and maintainability of the rewrite rules database. The database of rules represents the modelling knowledge acquired after analysis of expert models. This database must be easily extensible to best benefit from the active research on constraint modelling. Third, the automated modelling system Conjure is implemented as a realisation of these ideas; having an implementation enables empirical testing of the quality of generated models. The ease with which rewrite rules can be encoded to produce good models is shown. Furthermore, thanks to the generality of the system, one needs to add a very small number of rules to encode many transformations. Finally, the work is evaluated by comparing the generated models to expert models found in the literature for a wide variety of benchmark problems. This evaluation confirms the hypothesis that expert models can be automatically generated starting from high level problem specifications. An method of automatically identifying good models is also presented. In summary, this thesis presents a framework to enable the automatic generation of efficient constraint models from problem specifications. It provides a pleasant environment for both problem owners and modelling experts. Problem owners are presented with a fully automated constraint solution process, once they have a precise description of their problem. Modelling experts can now encode their precious modelling expertise as rewrite rules instead of merely modelling a single problem; resulting in reusable constraint modelling knowledge.},
  author = {Ozgur Akgun},
  year = {2014},
  language = {English},
  publisher = {University of St Andrews},
  url = {http://hdl.handle.net/10023/6547},
}

@inproceedings{ec7a3b357c8b4af4ba335c281ea87ba3,
  title = {An Automated Constraint Modelling and Solving Toolchain},
  author = {Ozgur Akgun and Frisch, {Alan M} and Gent, {Ian Philip} and Hussain, {Bilal Syed} and Jefferson, {Christopher Anthony} and Lars Kotthoff and Miguel, {Ian James} and Nightingale, {Peter William}},
  year = {2013},
  language = {English},
  booktitle = {The Twentieth Automated Reasoning Workshop (ARW 2013)},
  url = {http://staff.computing.dundee.ac.uk/katya/arw13/papers/paper_13.pdf},
}

@inproceedings{dfc0e4b721844d9d801fb27c264086ff,
  title = {Automated Modelling and Model Selection in Constraint Programming: Current Achievements and Future Directions},
  abstract = {In attacking the modelling bottleneck, we present current achievements in automated model generation and selection in constraint programming (CP). We also discuss promising future directions in automated model selection, which we believe are of key importance in enabling successful automated modelling in CP.},
  author = {Ozgur Akgun and Frisch, {Alan M} and Jefferson, {Christopher Anthony} and Miguel, {Ian James}},
  year = {2013},
  language = {English},
  booktitle = {COSpeL: The first Workshop on Domain Specific Languages in Combinatorial Optimization (COSpeL 2013)},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.388.5906&rep=rep1&type=pdf},
}

@inproceedings{066dfdbd563a40c68df2eaae83a342cd,
  title = {Automated Symmetry Breaking and Model Selection in Conjure},
  abstract = {Constraint modelling is widely recognised as a key bottleneck in applying constraint solving to a problem of interest. The Conjure automated constraint modelling system addresses this problem by automatically refining constraint models from problem specifications written in the Essence language. Essence provides familiar mathematical concepts like sets, functions and relations nested to any depth. To date, Conjure has been able to produce a set of alternative model kernels (i.e. without advanced features such as symmetry breaking or implied constraints) for a given specification. The first contribution of this paper is a method by which Conjure can break symmetry in a model as it is introduced by the modelling process. This works at the problem class level, rather than just individual instances, and does not require an expensive detection step after the model has been formulated. This allows Conjure to produce a higher quality set of models. A further limitation of Conjure has been the lack of a mechanism to select among the models it produces. The second contribution of this paper is to present two such mechanisms, allowing effective models to be chosen automatically.},
  author = {Ozgur Akgun and Frisch, {Alan M} and Gent, {Ian Philip} and Hussain, {Bilal Syed} and Jefferson, {Christopher Anthony} and Lars Kotthoff and Miguel, {Ian James} and Nightingale, {Peter William}},
  year = {2013},
  doi = {10.1007/978-3-642-40627-0_11},
  language = {English},
  booktitle = {The Nineteenth International Conference on Principles and Practice of Constraint Programming (CP 2013)},
}

@inproceedings{ed9a3c921f40425b964217636a5af521,
  title = {Extensible Automated Constraint Modelling},
  abstract = {In constraint solving, a critical bottleneck is the formulation of an effective constraint model of a given problem. The CONJURE system described in this paper, a substantial step forward over prototype versions of CONJURE previously reported, makes a valuable contribution to the automation of constraint modelling by automatically producing constraint models from their specifications in the abstract constraint specification language ESSENCE. A set of rules is used to refine an abstract specification into a concrete constraint model. We demonstrate that this set of rules is readily extensible to increase the space of possible constraint models CONJURE can produce. Our empirical results confirm that CONJURE can reproduce successfully the kernels of the constraint models of 32 benchmark problems found in the literature.},
  author = {Ozgur Akgun and Miguel, {Ian James} and Jefferson, {Christopher Anthony} and Frisch, {Alan M.} and Brahim Hnich},
  year = {2011},
  language = {English},
  isbn = {978-157735508-3},
  pages = {4--11},
  booktitle = {Twenty-Fifth AAAI Conference on Artificial Intelligence (AAAI 2011)},
  publisher = {AAAI Press},
  url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3687},
  note = {The Twenty-Fifth AAAI Conference on Artificial Intelligence and the 23rd Innovative Applications of Artificial Intelligence Conference, AAAI-11 / IAAI-11 ; Conference date: 07-08-2011 Through 11-08-2011},
}

@inproceedings{ecb8ca42a8eb4098a685daa84c821da3,
  title = {The Open Stacks Problem: An automated modelling case study},
  author = {Ozgur Akgun and Miguel, {Ian James} and Jefferson, {Christopher Anthony}},
  year = {2011},
  language = {English},
  pages = {15},
  booktitle = {ERCIM Workshop on Constraint Solving and Constraint Logic Programming (CSCLP 2011)},
  url = {https://web.archive.org/web/20200709224031/https://csclp2011.cs.st-andrews.ac.uk/csclp2011proceedings.pdf#page=21},
}

@inbook{aee46a0c83b14b25b687813677855191,
  title = {Conjure Revisited: Towards Automated Constraint Modelling},
  abstract = {Automating the constraint modelling process is one of thekey challenges facing the constraints field, and one of the principal obstaclespreventing widespread adoption of constraint solving. This paperfocuses on the refinement-based approach to automated modelling, wherea user specifies a problem in an abstract constraint specification languageand it is then automatically refined into a constraint model. In particular,we revisit the Conjure system that first appeared in prototype formin 2005 and present a new implementation with a much greater coverageof the specification language Essence},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Frisch, {Alan M} and Brahim Hnich and Jefferson, {Christopher Anthony} and Miguel, {Ian James}},
  year = {2010},
  language = {English},
  booktitle = {The Nineth Workshop on Constraint Modelling and Reformulation (ModRef 2010)},
  url = {https://it.uu.se/research/group/astra/ModRef10/papers/Ozgur%20Akgun,%20Alan%20Frisch,%20Brahim%20Hnich,%20Chris%20Jefferson%20and%20Ian%20Miguel.%20%20Conjure%20Revisited,%20Towards%20Automated%20Constraint%20Modelling%20-%20ModRef%202010.pdf},
}

@inproceedings{586c88ee08c1404391bc48c26d66e523,
  title = {Refining Portfolios of Constraint Models with Conjure},
  abstract = {Modelling is one of the key challenges in Constraint Programming(CP). There are many ways in which to model a given problem.The model chosen has a substantial effect on the solving efficiency. Itis difficult to know what the best model is. To overcome this problem wetake a portfolio approach: Given a high level specification of a combinatorialproblem, we employ non-deterministic rewrite techniques to obtaina portfolio of constraint models. The specification language (Essence)does not require humans to make modelling decisions; therefore it helpsus remove the modelling bottleneck.},
  author = {Ozgur Akgun},
  year = {2010},
  language = {English},
  pages = {1--6},
  booktitle = {The Sixteenth International Conference on Principles and Practice of Constraint Programming, Doctoral Program (CP 2010)},
  url = {https://arxiv.org/pdf/1109.1774.pdf},
}


