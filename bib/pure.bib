@inproceedings{cp2022hcicp,
  title = {Understanding how people approach constraint modelling and solving},
  author = {Ruth Hoffmann and Xu Zhu and Miguel Nacenta and Özgür Akgün},
  year = {2022},
  booktitle = {Twenty-Eighth International Conference on Principles and Practice of Constraint Programming (CP 2022)},
  url = {https://cp2022.a4cp.org},
}

@inproceedings{cp2022autoig,
  title = {A Framework for Generating Informative Benchmark Instances},
  author = {Nguyen Dang and Özgür Akgün and Joan Espasa and Ian Miguel and Peter Nightingale},
  year = {2022},
  booktitle = {Twenty-Eighth International Conference on Principles and Practice of Constraint Programming (CP 2022)},
  url = {https://cp2022.a4cp.org},
}


@article{10da0af044e1456383ca5e04ab7d7cb3,
  title = {Enumeration of set-theoretic solutions to the Yang-Baxter equation},
  abstract = {We use Constraint Satisfaction methods to enumerate and construct set-theoretic solutions to the Yang-Baxter equation of small size. We show that there are 321931 involutive solutions of size nine, 4895272 involutive solutions of size ten and 422449480 non-involutive solution of size eight. Our method is then used to enumerate non-involutive biquandles. },
  keywords = {math.GR, math.QA, 16T25},
  author = {Özgür Akgün and Martín Mereb and Leandro Vendramin},
  note = {13 pages, 8 tables. Accepted for publication in Mathematics of Computation},
  year = {2022},
  doi = {10.1090/mcom/3696},
  url = {https://www.ams.org/journals/mcom/0000-000-00/S0025-5718-2022-03696-6/},
  language = {English},
  journal = {Mathematics of Computation},
  issn = {0025-5718},
  publisher = {American Mathematical Society},
}

@inproceedings{36323c314240451291f8377d5a4f9668,
  title = {Towards reformulating Essence specifications for robustness},
  abstract = {The Essence language allows a user to specify a constraint problem at a level of abstraction above that at which constraint modelling decisions are made. Essence specifications are refined into constraint models using the Conjure automated modelling tool, which employs a suite of refinement rules. However, Essence is a rich language in which there are many equivalent ways to specify a given problem. A user may therefore omit the use of domain attributes or abstract types, resulting in fewer refinement rules being applicable and therefore a reduced set of output models from which to select. This paper addresses the problem of recovering this information automatically to increase the robustness of the quality of the output constraint models in the face of variation in the input Essence specification. We present reformulation rules that can change the type of a decision variable or add attributes that shrink its domain. We demonstrate the efficacy of this approach in terms of the quantity and quality of models Conjure can produce from the transformed specification compared with the original. },
  author = {{\"O}zg{\"u}r Akg{\"u}n and Frisch, {Alan M.} and Gent, {Ian P.} and Christopher Jefferson and Ian Miguel and Peter Nightingale and Salamon, {Andr{\'a}s Z.}},
  note = {Funding: This research was supported by the UK EPSRC grants EP/K015745/1 and EP/V027182/1. Chris Jefferson is a University Research Fellow funded by the Royal Society; The 20th workshop on Constraint Modelling and Reformulation (ModRef), ModRef ; Conference date: 25-10-2021 Through 25-10-2021},
  year = {2021},
  month = {oct},
  day = {25},
  language = {English},
  booktitle = {ModRef 2021 - The 20th workshop on Constraint Modelling and Reformulation (ModRef)},
  url = {https://modref.github.io/ModRef2021.html},
}

@inproceedings{d586090d1c36443da26e5656438fc8ab,
  title = {Finding subgraphs with side constraints},
  abstract = {The subgraph isomorphism problem is to find a small “pattern” graph inside a larger “target” graph. There are excellent dedicated solvers for this problem, but they require substantial programming effort to handle the complex side constraints that often occur in practical applications of the problem; however, general purpose constraint solvers struggle on more difficult graph instances. We show how to combine the state of the art Glasgow Subgraph Solver with the Minion constraint programming solver to get a “subgraphs modulo theories” solver that is both performant and flexible. We also show how such an approach can be driven by the Essence high level modelling language, giving ease of modelling and prototyping to non-expert users. We give practical examples involving temporal graphs, typed graphs from software engineering, and costed subgraph isomorphism problems.},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Jessica Enright and Christopher Jefferson and Ciaran McCreesh and Patrick Prosser and Steffen Zschaler},
  note = {This research was supported by the Engineering and Physical Sciences Research Council [grant number EP/P026842/1].; 18th International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research, CPAIOR 2021 ; Conference date: 05-07-2021 Through 08-07-2021},
  year = {2021},
  month = {jul},
  day = {5},
  doi = {10.1007/978-3-030-78230-6_22},
  language = {English},
  isbn = {9783030782290},
  series = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  pages = {348--364},
  editor = {Stuckey, {Peter J.}},
  booktitle = {Integration of Constraint Programming, Artificial Intelligence, and Operations Research - 18th International Conference, CPAIOR 2021, Proceedings},
  address = {Netherlands},
}

@inproceedings{ae7faa006d3d4ecbb19a14df924e2088,
  title = {Efficient incremental modelling and solving},
  abstract = {In various scenarios, a single phase of modelling and solving is either not sufficient or not feasible to solve the problem at hand. A standard approach to solving AI planning problems, for example, is to incrementally extend the planning horizon and solve the problem of trying to find a plan of a particular length. Indeed, any optimization problem can be solved as a sequence of decision problems in which the objective value is incrementally updated. Another example is constraint dominance programming (CDP), in which search is organized into a sequence of levels. The contribution of this work is to enable a native interaction between SAT solvers and the automated modelling system Savile Row to support efficient incremental modelling and solving. This allows adding new decision variables, posting new constraints and removing existing constraints (via assumptions) between incremental steps. Two additional benefits of the native coupling of modelling and solving are the ability to retain learned information between SAT solver calls and to enable SAT assumptions, further improving flexibility and efficiency. Experiments on one optimisation problem and five pattern mining tasks demonstrate that the native interaction between the modelling system and SAT solver consistently improves performance significantly. },
  keywords = {Constraint programming, Constraint modelling, Incremental solving, Constraint optimization, Planning, Data mining, Itemset mining, Pattern mining, Dominance programming},
  author = {G{\"o}kberk Ko{\c c}ak and {\"O}zg{\"u}r Akg{\"u}n and Nguyen Dang and Ian Miguel},
  note = {Funding: This work is supported by EPSRC grant EP/P015638/1. Nguyen Dang is a Leverhulme Trust Early Career Fellow (ECF-2020-168).; The 19th workshop on Constraint Modelling and Reformulation (ModRef), ModRef ; Conference date: 07-09-2020 Through 07-09-2020},
  year = {2020},
  month = {sep},
  day = {7},
  language = {English},
  booktitle = {ModRef 2020 - The 19th workshop on Constraint Modelling and Reformulation},
  url = {https://modref.github.io/ModRef2020.html},
}

@inproceedings{c651056c471d41e3af7df7f4ca9b23ec,
  title = {Exploring instance generation for automated planning},
  abstract = {Many of the core disciplines of artificial intelligence have sets of standard benchmark problems well known and widely used by the community when developing new algorithms. Constraint programming and automated planning are examples of these areas, where the behaviour of a new algorithm is measured by how it performs on these instances. Typically the efficiency of each solving method varies not only between problems, but also between instances of the same problem. Therefore, having a diverse set of instances is crucial to be able to effectively evaluate a new solving method. Current methods for automatic generation of instances for Constraint Programming problems start with a declarative model and search for instances with some desired attributes, such as hardness or size. We first explore the difficulties of adapting this approach to generate instances starting from problem specifications written in PDDL, the de-facto standard language of the automated planning community. We then propose a new approach where the whole planning problem description is modelled using Essence, an abstract modelling language that allows expressing high-level structures without committing to a particular low level representation in PDDL. },
  author = {{\"O}zg{\"u}r Akg{\"u}n and Nguyen Dang and Joan Espasa and Ian Miguel and Salamon, {Andr{\'a}s Z.} and Christopher Stone},
  note = {Funding: This work is supported by EPSRC grant EP/P015638/1. Nguyen Dang is a Leverhulme Early Career Fellow.},
  year = {2020},
  month = {sep},
  day = {7},
  language = {English},
  booktitle = {ModRef 2020 - The 19th workshop on Constraint Modelling and Reformulation},
}

@inproceedings{d868096c9b0a428f83a1d508f98736a5,
  title = {Towards portfolios of streamlined constraint models: a case study with the balanced academic curriculum problem},
  abstract = {Augmenting a base constraint model with additional constraints can strengthen the inferences made by a solver and therefore reduce search effort. We focus on the automatic addition of streamliner constraints, derived from the types present in an abstract Essence specification of a problem class of interest, which trade completeness for potentially very significant reduction in search. The refinement of streamlined Essence specifications into constraint models suitable for input to constraint solvers gives rise to a large number of modelling choices in addition to those required for the base Essence specification. Previous automated streamlining approaches have been limited in evaluating only a single default model for each streamlined specification. In this paper we explore the effect of model selection in the context of streamlined specifications. We propose a new best-first search method that generates a portfolio of Pareto Optimal streamliner-model combinations by evaluating for each streamliner a portfolio of models to search and explore the variability in performance and find the optimal model. Various forms of racing are utilised to constrain the computational cost of training. },
  keywords = {Constraint programming, Streamliners},
  author = {Patrick Spracklen and Nguyen Dang and {\"O}zg{\"u}r Akg{\"u}n and Ian Miguel},
  note = {Funding: This work is supported by EPSRC grant EP/P015638/1 and used the Cirrus UK National Tier-2 HPC Service at EPCC (http://www.cirrus.ac.uk) funded by the University of Edinburgh and EPSRC (EP/P020267/1). Nguyen Dang is a Leverhulme Early Career Fellow.; The 19th workshop on Constraint Modelling and Reformulation (ModRef), ModRef ; Conference date: 07-09-2020 Through 07-09-2020},
  year = {2020},
  month = {sep},
  day = {7},
  language = {English},
  booktitle = {ModRef 2020 - The 19th workshop on Constraint Modelling and Reformulation},
  url = {https://modref.github.io/ModRef2020.html},
}

@inproceedings{17528eb080e74cb6ac61a8387e9ac515,
  title = {Exploiting incomparability in solution dominance: improving general purpose constraint-based mining},
  abstract = {In data mining, finding interesting patterns is a challenging task. Constraint-based mining is a well-known approach to this, and one for which constraint programming has been shown to be a well-suited and generic framework. Constraint dominance programming (CDP) has been proposed as an extension that can capture an even wider class of constraint-based mining problems, by allowing us to compare relations between patterns. In this paper we improve CDP with the ability to specify an incomparability condition. This allows us to overcome two major shortcomings of CDP: finding dominated solutions that must then be filtered out after search, and unnecessarily adding dominance blocking constraints between incomparable solutions. We demonstrate the efficacy of our approach by extending the problem specification language ESSENCE and implementing it in a solver-independent manner on top of the constraint modelling tool CONJURE. Our experiments on pattern mining tasks with both a CP solver and a SAT solver show that using the incomparability condition during search significantly improves the efficiency of dominance programming and reduces (and often eliminates entirely) the need for post-processing to filter dominated solutions.},
  author = {Gokberk Kocak and Ozgur Akgun and Tias Guns and Miguel, {Ian James}},
  year = {2020},
  month = {aug},
  day = {29},
  doi = {10.3233/FAIA200110},
  language = {English},
  isbn = {9781643681009},
  series = {Frontiers in artificial intelligence and applications},
  publisher = {IOS Press},
  pages = {331--338},
  editor = {{De Giacomo}, Giuseppe and Alejandro Catala and Bistra Dilkina and Michela Milano and Sen{\'e}n Barro and Alberto Bugar{\'i}n and J{\'e}r{\^o}me Lang},
  booktitle = {ECAI 2020},
  address = {Netherlands},
  note = {24th European Conference on Artificial Intelligence (ECAI2020), ECAI2020 ; Conference date: 29-08-2020 Through 02-09-2020},
  url = {https://ecai2020.eu/},
}

@article{921447ff03114bdea8db07407ee51887,
  title = {Linking Scottish vital event records using family groups},
  abstract = {The reconstitution of populations through linkage of historical records is a powerful approach to generate longitudinal historical microdata resources of interest to researchers in various fields. Here we consider automated linking of the vital events recorded in the civil registers of birth, death and marriage compiled in Scotland, to bring together the various records associated with the demographic events in the life course of each individual in the population. From the histories, the genealogical structure of the population can then be built up. Rather than apply standard linkage techniques to link the individuals on the available certificates, we explore an alternative approach, inspired by the family reconstitution techniques adopted by historical demographers, in which the births of siblings are first linked to form family groups, after which intergenerational links between families can be established. We report a small-scale evaluation of this approach, using two district-level data sets from Scotland in the late nineteenth century, for which sibling links have already been created by demographers. We show that quality measures of up to 83% can be achieved on these data sets (using F-Measure, a combination of precision and recall). In the future, we intend to compare the results with a standard linkage approach and to investigate how these various methods may be used in a project which aims to link the entire Scottish population from 1856 to 1973.},
  keywords = {Scottish vital event records, Record linkage, Linkage methods, Group linkage, Population reconstruction, Digitising Scotland},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Alan Dearle and Kirby, {Graham Njal Cameron} and Eilidh Garrett and Dalton, {Thomas Stanley} and Peter Christen and Dibben, {Christopher John Lloyd} and Williamson, {Lee Emma Palmer}},
  year = {2020},
  month = {apr},
  day = {2},
  doi = {10.1080/01615440.2019.1571466},
  language = {English},
  volume = {Latest articles},
  journal = {Historical Methods: a Journal of Quantitative and Interdisciplinary History},
  issn = {0161-5440},
  publisher = {Routledge Taylor & Francis Group},
}

@inproceedings{04b6f9716a204104b26a8a664d101d92,
  title = {Discriminating instance generation from abstract specifications: a case study with CP and MIP},
  abstract = {We extend automatic instance generation methods to allow cross-paradigm comparisons. We demonstrate that it is possible to completely automate the search for benchmark instances that help to discriminate between solvers. Our system starts from a high level human-provided problem specification, which is translated into a specification for valid instances. We use the automated algorithm configuration tool irace to search for instances, which are translated into inputs for both MIP and CP solvers by means of the Conjure, Savile Row, and MiniZinc tools. These instances are then solved by CPLEX and Chuffed, respectively. We constrain our search for instances by requiring them to exhibit a significant advantage for MIP over CP, or vice versa. Experimental results on four optimisation problem classes demonstrate the effectiveness of our method in identifying instances that highlight differences in performance of the two solvers.},
  keywords = {Constraint Programming, Instance generation, MIP},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Nguyen Dang and Ian Miguel and Salamon, {Andr{\'a}s Z.} and Patrick Spracklen and Christopher Stone},
  note = {This work is supported by EPSRC grant EP/P015638/1 and used the Cirrus UK National Tier-2 HPC Service at EPCC funded by the University of Edinburgh and EPSRC (EP/P020267/1).; 17th International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research, CPAIOR 2020 ; Conference date: 21-09-2020 Through 24-09-2020},
  year = {2020},
  doi = {10.1007/978-3-030-58942-4_3},
  language = {English},
  isbn = {9783030589417},
  series = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  pages = {41--51},
  editor = {Emmanuel Hebrard and Nysret Musliu},
  booktitle = {Integration of Constraint Programming, Artificial Intelligence, and Operations Research},
  address = {Netherlands},
  url = {https://cpaior2020.dbai.tuwien.ac.at/},
}

@inproceedings{5d418525265e4c2189449a84ef9db61e,
  title = {Effective encodings of constraint programming models to SMT},
  abstract = {Satisfiability Modulo Theories (SMT) is a well-established methodology that generalises propositional satisfiability (SAT) by adding support for a variety of theories such as integer arithmetic and bit-vector operations. SMT solvers have made rapid progress in recent years. In part, the efficiency of modern SMT solvers derives from the use of specialised decision procedures for each theory. In this paper we explore how the Essence Prime constraint modelling language can be translated to the standard SMT-LIB language. We target four theories: bit-vectors (QF_BV), linear integer arithmetic (QF_LIA), non-linear integer arithmetic (QF_NIA), and integer difference logic (QF_IDL). The encodings are implemented in the constraint modelling tool Savile Row. In an extensive set of experiments, we compare our encodings for the four theories, showing some notable differences and complementary strengths. We also compare our new encodings to the existing work targeting SMT and SAT, and to a well-established learning CP solver. Our two proposed encodings targeting the theory of bit-vectors (QF_BV) both substantially outperform earlier work on encoding to QF_BV on a large and diverse set of problem classes.},
  keywords = {Constraint modelling, SMT, Automated reformulation},
  author = {Ewan Davidson and Ozgur Akgun and {Espasa Arxer}, Joan and Peter Nightingale},
  note = {Funding: UK EPSRC grant EP/P015638/1.; 26th International Conference on Principles and Practice of Constraint Programming (CP 2020), CP 2020 ; Conference date: 07-09-2020 Through 11-09-2020},
  year = {2020},
  doi = {10.1007/978-3-030-58475-7_9},
  language = {English},
  isbn = {9783030584740},
  series = {Lecture Notes in Computer Science (Programming and Software Engineering)},
  publisher = {Springer},
  pages = {143--159},
  editor = {Helmut Simonis},
  booktitle = {Twenty-Sixth International Conference on Principles and Practice of Constraint Programming (CP 2020)},
  address = {Netherlands},
  url = {https://cp2020.a4cp.org/},
}

@inproceedings{e74974988ec540bab669443ad10422b3,
  title = {Towards improving solution dominance with incomparability conditions: a case-study using Generator Itemset Mining},
  abstract = {Finding interesting patterns is a challenging task in data mining. Constraint based mining is a well-known approach to this, and one for which constraint programming has been shown to be a well-suited and generic framework.Dominance programming has been proposed as an extension that can capture aneven wider class of constraint-based mining problems, by allowing to comparerelations between patterns. In this paper, in addition to specifying a dominancerelation, we introduce the ability to specify an incomparability condition. Usingthese two concepts we devise a generic framework that can do a batch-wise searchthat avoids checking incomparable solutions. We extend the ESSENCE languageand underlying modelling pipeline to support this. We use generator itemset mining problem as a test case and give a declarative specification for that. We alsopresent preliminary experimental results on this specific problem class with a CPsolver backend to show that using the incomparability condition during searchcan improve the efficiency of dominance programming and reduces the need forpost-processing to filter dominated solutions.},
  keywords = {Constraint programming, Constraint modelling, Data mining, Itemset mining, Pattern mining, Dominance programming},
  author = {Gokberk Kocak and {\"O}zg{\"u}r Akg{\"u}n and Ian Miguel and Tias Guns},
  note = {Funding: EPSRC (EP/P015638/1).; 25th International Conference on Principles and Practice of Constraint Programming (CP 2019), CP 2019 ; Conference date: 30-09-2019 Through 04-10-2019},
  year = {2019},
  month = {sep},
  day = {30},
  language = {English},
  booktitle = {The 18th workshop on Constraint Modelling and Reformulation (ModRef 2019), Proceedings},
  url = {http://cp2019.a4cp.org},
}

@article{6d13ed10041c43c49530b2b55d688e91,
  title = {Solving computational problems in the theory of word-representable graphs},
  abstract = {A simple graph G = (V, E) is word-representable if there exists a word w over the alphabet V such that letters x and y alternate in w iff xy ∈ E. Word-representable graphs generalize several important classes of graphs. A graph is word-representable if it admits a semi-transitive orientation. We use semi-transitive orientations to enumerate connected non-word-representable graphs up to the size of 11 vertices, which led to a correction of a published result. Obtaining the enumeration results took 3 CPU years of computation.Also, a graph is word-representable if it is k-representable for some k, that is, if it can be represented using k copies of each letter. The minimum such k for a given graph is called graph's representation number. Our computational results in this paper not only include distribution of k-representable graphs on at most 9 vertices, but also have relevance to a known conjecture on these graphs. In particular, we find a new graph on 9 vertices with high representation number. Also, we prove that a certain graph has highest representation number among all comparability graphs on odd number of vertices.Finally, we introduce the notion of a k-semi-transitive orientation refining the notion of a semi-transitive orientation, and show computationally that the refinement is not equivalent to the original definition, unlike the equivalence of k-representability and word-representability. },
  keywords = {Word-representable graph, Representation number, Enumeration, Semi-transitive orientation, k-semi-transitive orientation},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Gent, {Ian P.} and Sergey Kitaev and Hans Zantema},
  year = {2019},
  month = {feb},
  day = {24},
  language = {English},
  volume = {22},
  journal = {Journal of Integer Sequences},
  issn = {1530-7638},
  publisher = {University of Waterloo},
  number = {2},
  url = {https://cs.uwaterloo.ca/journals/JIS/VOL22/Kitaev/kitaev11.html},
}

@article{4b54ed2e79924712bcf8e6fea3211d72,
  title = {How people visually represent discrete constraint problems},
  abstract = {Problems such as timetabling or personnel allocation can be modeled and solved using discrete constraint programming languages. However, while existing constraint solving software solves such problems quickly in many cases, these systems involve specialized languages that require significant time and effort to learn and apply. These languages are typically text-based and often difficult to interpret and understand quickly, especially for people without engineering or mathematics backgrounds. Visualization could provide an alternative way to model and understand such problems. Although many visual programming languages exist for procedural languages, visual encoding of problem specifications has not received much attention. Future problem visualization languages could represent problem elements and their constraints unambiguously, but without unnecessary cognitive burdens for those needing to translate their problem's mental representation into diagrams. As a first step towards such languages, we executed a study that catalogs how people represent constraint problems graphically. We studied three groups with different expertise: non-computer scientists, computer scientists and constraint programmers and analyzed their marks on paper (e.g., arrows), gestures (e.g., pointing) and the mappings to problem concepts (e.g., containers, sets). We provide foundations to guide future tool designs allowing people to effectively grasp, model and solve problems through visual representations.},
  keywords = {Problem visualization, Problem modeling, Problem solving, Constraint programming, Visual programming languages},
  author = {Xu Zhu and Miguel Nacenta and {\"O}zg{\"u}r Akg{\"u}n and Nightingale, {Peter William}},
  note = {Funding: This work is supported by EPSRC grants DTG1796157 and EP/P015638/1.},
  year = {2019},
  month = {jan},
  day = {24},
  doi = {10.1109/TVCG.2019.2895085},
  language = {English},
  volume = {26},
  pages = {2603 -- 2619},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  issn = {1077-2626},
  publisher = {IEEE Computer Society},
  number = {8},
}

@article{1a41bce3240a48d8a8c1b977f34215ea,
  title = {Cloud benchmarking for maximising performance of scientific applications},
  abstract = {How can applications be deployed on the cloud to achieve maximum performance? This question is challenging to address with the availability of a wide variety of cloud Virtual Machines (VMs) with different performance capabilities. The research reported in this paper addresses the above question by proposing a six step benchmarking methodology in which a user provides a set of weights that indicate how important memory, local communication, computation and storage related operations are to an application. The user can either provide a set of four abstract weights or eight fine grain weights based on the knowledge of the application. The weights along with benchmarking data collected from the cloud are used to generate a set of two rankings - one based only on the performance of the VMs and the other takes both performance and costs into account. The rankings are validated on three case study applications using two validation techniques. The case studies on a set of experimental VMs highlight that maximum performance can be achieved by the three top ranked VMs and maximum performance in a cost-effective manner is achieved by at least one of the top three ranked VMs produced by the methodology.},
  keywords = {Cloud benchmarking, Cloud performance, Benchmarking methodology, Cloud ranking},
  author = {Blesson Varghese and Ozgur Akgun and Miguel, {Ian James} and Thai, {Long Thanh} and Barker, {Adam David}},
  note = {This research was pursued under the EPSRC grant, EP/K015745/1, a Royal Society Industry Fellowship and an AWS Education Research grant.},
  year = {2019},
  month = {jan},
  day = {1},
  doi = {10.1109/TCC.2016.2603476},
  language = {English},
  volume = {7},
  pages = {170--182},
  journal = {IEEE Transactions on Cloud Computing},
  issn = {2168-7161},
  publisher = {IEEE},
  number = {1},
}

@inproceedings{ce8510eb25d34db48cd8c3c7682edfdf,
  title = {Automatic streamlining for constrained optimisation},
  abstract = {Augmenting a base constraint model with additional constraints can strengthen the inferences made by a solver and therefore reduce search effort. We focus on the automatic addition of streamliner constraints, which trade completeness for potentially very significant reduction in search. Recently an automated approach has been proposed, which produces streamliners via a set of streamliner generation rules. This existing automated approach to streamliner generation has two key limitations. First, it outputs a single streamlined model. Second, the approach is limited to satisfaction problems. We remove both limitations by providing a method to produce automatically a portfolio of streamliners, each representing a different balance between three criteria: how aggressively the search space is reduced, the proportion of training instances for which the streamliner admitted at least one solution, and the average reduction in quality of the objective value versus the unstreamlined model. In support of our new method, we present an automated approach to training and test instance generation, and provide several approaches to the selection and application of the streamliners from the portfolio. Empirical results demonstrate drastic improvements both to the time required to find good solutions early and to prove optimality on three problem classes.},
  keywords = {Constraint programming, Streamliners},
  author = {Patrick Spracklen and Nguyen Dang and Ozgur Akgun and Miguel, {Ian James}},
  note = {Funding: UK EPSRC grant EP/P015638/1.; 25th International Conference on Principles and Practice of Constraint Programming (CP 2019), CP 2019 ; Conference date: 30-09-2019 Through 04-10-2019},
  year = {2019},
  doi = {10.1007/978-3-030-30048-7_22},
  language = {English},
  isbn = {9783030300470},
  series = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  pages = {366--383},
  editor = {Thomas Schiex and {de Givry}, Simon},
  booktitle = {Twenty-Fifth International Conference on Principles and Practice of Constraint Programming (CP 2019)},
  address = {Netherlands},
  url = {http://cp2019.a4cp.org},
}

@inproceedings{b1d1e65f1ecf4431bc087cff9ef9b9dd,
  title = {Instance generation via generator instances},
  abstract = {Access to good benchmark instances is always desirable when developing new algorithms, new constraint models, or when comparing existing ones. Hand-written instances are of limited utility and are time-consuming to produce. A common method for generating instances is constructing special purpose programs for each class of problems. This can be better than manually producing instances, but developing such instance generators also has drawbacks. In this paper, we present a method for generating graded instances completely automatically starting from a class-level problem specification. A graded instance in our present setting is one which is neither too easy nor too difficult for a given solver. We start from an abstract problem specification written in the Essence language and provide a system to transform the problem specification, via automated type-specific rewriting rules, into a new abstract specification which we call a generator specification. The generator specification is itself parameterised by a number of integer parameters; these are used to characterise a certain region of the parameter space. The solutions of each such generator instance form valid problem instances. We use the parameter tuner irace to explore the space of possible generator parameters, aiming to find parameter values that yield graded instances. We perform an empirical evaluation of our system for five problem classes from CSPlib, demonstrating promising results.},
  keywords = {Automated modelling, Instance generation, Parameter tuning},
  author = {Ozgur Akgun and Nguyen Dang and Miguel, {Ian James} and Salamon, {Andr{\'a}s Z.} and Stone, {Christopher Luciano}},
  note = {Funding: UK EPSRC grant EP/P015638/1.; 25th International Conference on Principles and Practice of Constraint Programming (CP 2019), CP 2019 ; Conference date: 30-09-2019 Through 04-10-2019},
  year = {2019},
  doi = {10.1007/978-3-030-30048-7_1},
  language = {English},
  isbn = {9783030300470},
  series = {Lecture Notes in Computer Science (Programming and Software Engineering)},
  publisher = {Springer},
  pages = {3--19},
  editor = {Thomas Schiex and {de Givry}, Simon},
  booktitle = {Twenty-Fifth International Conference on Principles and Practice of Constraint Programming (CP 2019)},
  address = {Netherlands},
  url = {http://cp2019.a4cp.org},
}

@inproceedings{921a03b374654acdb3cf8b608e1ef86a,
  title = {Closed frequent itemset mining with arbitrary side constraints},
  abstract = {Frequent itemset mining (FIM) is a method for finding regularities in transaction databases. It has several application areas, such as market basket analysis, genome analysis, and drug design. Finding frequent itemsets allows further analysis to focus on a small subset of the data. For large datasets the number of frequent itemsets can also be very large, defeating their purpose. Therefore, several extensions to FIM have been studied, such as adding high-utility (or low-cost) constraints and only finding closed (or maximal) frequent itemsets. This paper presents a constraint programming based approach that combines arbitrary side constraints with closed frequent itemset mining. Our approach allows arbitrary side constraints to be expressed in a high level and declarative language which is then translated automatically for efficient solution by a SAT solver. We compare our approach with state-of-the-art algorithms via the MiningZinc system (where possible) and show significant contributions in terms of performance and applicability.},
  keywords = {Data mining, Pattern mining, Frequent itemset mining, Closed frequent itemset mining, Constraint modelling},
  author = {Gokberk Kocak and Ozgur Akgun and Miguel, {Ian James} and Nightingale, {Peter William}},
  year = {2018},
  month = {nov},
  day = {17},
  doi = {10.1109/ICDMW.2018.00175},
  language = {English},
  isbn = {9781538692899},
  pages = {1224 -- 1232},
  editor = {Hanghang Tong and Li, {Zhenhui (Jessie)} and Feida Zhu and Jeffrey Yu},
  booktitle = {2018 IEEE International Conference on Data Mining Workshops (ICDMW)},
  publisher = {IEEE Computer Society},
  address = {United States},
  note = {Workshop on Optimization Based Techniques for Emerging Data Mining Problems (OEDM 2018), OEDM 2018 ; Conference date: 17-11-2018 Through 20-11-2018},
  url = {https://qizhiquan.github.io/OEDM-18/},
}

@inproceedings{8f516aac022d4bcdb34e5f63976bdd78,
  title = {Memory consistency models using constraints},
  abstract = {Memory consistency models (MCMs) are at the heart of concurrent programming. They represent the behaviour of concurrent programs at the chip level. To test these models small program snippets called litmus test are generated, which show allowed or forbidden behaviour of different MCMs. This paper is showcasing the use of constraint programming to automate the generation and testing of litmus tests for memory consistency models. We produce a few exemplary case studies for two MCMs, namely Sequential Consistency and Total Store Order. These studies demonstrate the flexibility of constrains programming in this context and lay foundation to the direct verification of MCMs against the software facing cache coherence protocols. },
  keywords = {Memory consistency, Concurrent programming, Litmus tests, Constraints programming, Modelling},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Ruth Hoffmann and Susmit Sarkar},
  year = {2018},
  month = {aug},
  day = {27},
  language = {English},
  booktitle = {The Seventeenth Workshop on Constraint Modelling and Reformulation (ModRef 2018), Proceedings},
  note = {24th International Conference on Principles and Practice of Constraint Programming (CP 2018), CP 2018 ; Conference date: 27-08-2018 Through 31-08-2018},
  url = {http://cp2018.a4cp.org/},
}

@inproceedings{56bd918cab3c4a66b6fae5b71f88b1b6,
  title = {Modelling Langford's Problem: a viewpoint for search},
  abstract = {The performance of enumerating all solutions to an instance of Langford's Problem is sensitive to the model and the search strategy. In this paper we compare the performance of a large variety of models, all derived from two base viewpoints. We empirically show that a channelled model with a static branching order on one of the viewpoints offers the best performance out of all the options we consider. Surprisingly, one of the base models proves very effective for propagation, while the other provides an effective means of stating a static search order. },
  author = {{\"O}zg{\"u}r Akg{\"u}n and Ian Miguel},
  note = {Funding: EPSRC (EP/P015638/1).; 24th International Conference on Principles and Practice of Constraint Programming (CP 2018), CP 2018 ; Conference date: 27-08-2018 Through 31-08-2018},
  year = {2018},
  month = {aug},
  day = {27},
  language = {English},
  booktitle = {The Seventeenth Workshop on Constraint Modelling and Reformulation (ModRef 2018), Proceedings},
  url = {http://cp2018.a4cp.org/},
}

@inproceedings{413b9d1324cf4826b5ea1a130eb96159,
  title = {A framework for constraint based local search using ESSENCE},
  abstract = {Structured Neighbourhood Search (SNS) is a framework for constraint-based local search for problems expressed in the Essence abstract constraint specification language. The local search explores a structured neighbourhood, where each state in the neighbourhood preserves a high level structural feature of the problem. SNS derives highly structured problem-specific neighbourhoods automatically and directly from the features of the ESSENCE specification of the problem. Hence, neighbourhoods can represent important structural features of the problem, such as partitions of sets, even if that structure is obscured in the low-level input format required by a constraint solver. SNS expresses each neighbourhood as a constrained optimisation problem, which is solved with a constraint solver. We have implemented SNS, together with automatic generation of neighbourhoods for high level structures, and report high quality results for several optimisation problems.},
  keywords = {Constraints and SAT: constraint satisfaction, Constraints and SAT: modeling; formulation, Constraints and SAT: constraint ptimisation, Constraints and SAT: Constraints: solvers and tools},
  author = {Ozgur Akgun and Attieh, {Saad Wasim A} and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William} and Salamon, {Andr{\'a}s Z.} and Patrick Spracklen and Wetter, {James Patrick}},
  note = {Funding: UK Engineering & Physical Sciences Research Council (EPSRC) grants EP/P015638/1and EP/P026842/1.; 27th International Joint Conference on Artificial Intelligence and the 23rd European Conference on Artificial Intelligence, IJCAI-ECAI-18 ; Conference date: 13-07-2018 Through 19-07-2018},
  year = {2018},
  month = {jul},
  day = {13},
  doi = {10.24963/ijcai.2018/173},
  language = {English},
  pages = {1242--1248},
  editor = {J{\'e}r{\^o}me Lang},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
  publisher = {International Joint Conferences on Artificial Intelligence},
  url = {https://www.ijcai-18.org/},
}

@conference{c4f1f1048f82470c9f62be594dd5a3a3,
  title = {Validating Synthetic Longitudinal Populations for evaluation of Population Data Linkage},
  abstract = {BackgroundGold-standard data to evaluate linkage algorithms are rare. Synthetic data have the advantage that all the true links are known. In the domain of population reconstruction, the ability to synthesize populations on demand, with varying characteristics, allows a linkage approach to be evaluated across a wide range of data. We have implemented ValiPop, a microsimulation model, for this purpose.ApproachValiPop can create many varied populations based upon sets of desired population statistics, thus allowing linkage algorithms to be evaluated across many populations, rather than across a limited number of real world gold-standard data sets.Given the potential interactions between different desired population statistics, the creation of a population does not necessarily imply that all desired population statistics have been met. To address this we have developed a statistical approach to validate the adherence of created populations to the desired statistics, using a generalized linear model.This talk will discuss the benefits of synthetic data for data linkage evaluation, the approach to validating created populations, and present the results of some initial linkage experiments using our synthetic data.},
  author = {Dalton, {Thomas Stanley} and Kirby, {Graham Njal Cameron} and Alan Dearle and Ozgur Akgun and MacKenzie, {Monique Lea}},
  year = {2018},
  month = {jun},
  day = {11},
  doi = {10.23889/ijpds.v3i2.504},
  language = {English},
}

@inproceedings{b170a1edfb03483a860a1e1482829944,
  title = {Automatic discovery and exploitation of promising subproblems for tabulation},
  abstract = {The performance of a constraint model can often be improved by converting a subproblem into a single table constraint. In this paper we study heuristics for identifying promising subproblems. We propose a small set of heuristics to identify common cases such as expressions that will propagate weakly. The process of discovering promising subproblems and tabulating them is entirely automated in the tool Savile Row. A cache is implemented to avoid tabulating equivalent subproblems many times. We give a simple algorithm to generate table constraints directly from a constraint expression in Savile Row. We demonstrate good performance on the benchmark problems used in earlier work on tabulation, and also for several new problem classes. },
  author = {Ozgur Akgun and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William} and Salamon, {Andr{\'a}s Z.}},
  note = {Funding: EP/P015638/1 and EP/P026842/1. Dr Jefferson holds a Royal Society University Research Fellowship.; 24th International Conference on Principles and Practice of Constraint Programming (CP 2018), CP 2018 ; Conference date: 27-08-2018 Through 31-08-2018},
  year = {2018},
  doi = {10.1007/978-3-319-98334-9_1},
  language = {English},
  isbn = {9783319983332},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  pages = {3--12},
  editor = {John Hooker},
  booktitle = {Twenty-Fourth International Conference on Principles and Practice of Constraint Programming (CP 2018)},
  address = {Netherlands},
  url = {http://cp2018.a4cp.org/},
}

@inproceedings{6eef5285c1a0471ebba55a9179298de8,
  title = {Automatic generation and selection of streamlined constraint models via Monte Carlo search on a model lattice},
  abstract = {Streamlined constraint reasoning is the addition of uninferred constraints to a constraint model to reduce the search space, while retaining at least one solution. Previously it has been established that it is possible to generate streamliners automatically from abstract constraint specifications in Essence and that effective combinations of streamliners can allow instances of much larger scale to be solved. A shortcoming of the previous approach was the crude exploration of the power set of all combinations using depth and breadth first search. We present a new approach based on Monte Carlo search over the lattice of streamlined models, which efficiently identifies effective streamliner combinations.},
  author = {Patrick Spracklen and Ozgur Akgun and Miguel, {Ian James}},
  note = {Funding: EPSRC EP/P015638/1.},
  year = {2018},
  doi = {10.1007/978-3-319-98334-9_24},
  language = {English},
  isbn = {9783319983332},
  series = {Lecture Notes in Computer Science (including subseries Programming and Software Engineering)},
  publisher = {Springer},
  pages = {362--372},
  editor = {John Hooker},
  booktitle = {Twenty-Fourth International Conference on Principles and Practice of Constraint Programming (CP 2018)},
  address = {Netherlands},
}

@inproceedings{53282bbae2084f9ab816728d0a72fd7b,
  title = {Metamorphic testing of constraint solvers},
  abstract = {Constraint solvers are complex pieces of software and are notoriously difficult to debug. In large part this is due to the difficulty of pinpointing the source of an error in the vast searches these solvers perform, since the effect of an error may only come to light long after the error is made. In addition, an error does not necessarily lead to the wrong result, further complicating the debugging process. A major source of errors in a constraint solver is the complex constraint propagation algorithms that provide the inference that controls and directs the search. In this paper we show that metamorphic testing is a principled way to test constraint solvers by comparing two different implementations of the same constraint. Specifically, specialised propagators for the constraint are tested against the general purpose table constraint propagator. We report on metamorphic testing of the constraint solver Minion. We demonstrate that the metamorphic testing method is very effective for finding artificial bugs introduced by random code mutation.},
  author = {Ozgur Akgun and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William}},
  note = {Funding: EPSRC EP/P015638/1 and EP/P026842/1. Dr Jefferson holds a Royal Society University Research Fellowship.; 24th International Conference on Principles and Practice of Constraint Programming (CP 2018), CP 2018 ; Conference date: 27-08-2018 Through 31-08-2018},
  year = {2018},
  doi = {10.1007/978-3-319-98334-9_46},
  language = {English},
  isbn = {9783319983332},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  pages = {727--736},
  editor = {John Hooker},
  booktitle = {Twenty-Fourth International Conference on Principles and Practice of Constraint Programming (CP 2018)},
  address = {Netherlands},
  url = {http://cp2018.a4cp.org/},
}

@inproceedings{2bf3e6cc02c540439b29015862135919,
  title = {Using metric space indexing for complete and efficient record linkage},
  abstract = {Record linkage is the process of identifying records that refer to the same real-world entities in situations where entity identifiers are unavailable. Records are linked on the basis of similarity between common attributes, with every pair being classified as a link or non-link depending on their similarity. Linkage is usually performed in a three-step process: first, groups of similar candidate records are identified using indexing, then pairs within the same group are compared in more detail, and finally classified. Even state-of-the-art indexing techniques, such as locality sensitive hashing, have potential drawbacks. They may fail to group together some true matching records with high similarity, or they may group records with low similarity, leading to high computational overhead. We propose using metric space indexing (MSI) to perform complete linkage, resulting in a parameter-free process combining indexing, comparison and classification into a single step delivering complete and efficient record linkage. An evaluation on real-world data from several domains shows that linkage using MSI can yield better quality than current indexing techniques, with similar execution cost, without the need for domain knowledge or trial and error to configure the process.},
  keywords = {Entity resolution, Data matching, Similarity search, Blocking},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Alan Dearle and Kirby, {Graham Njal Cameron} and Peter Christen},
  year = {2018},
  doi = {10.1007/978-3-319-93040-4_8},
  language = {English},
  isbn = {9783319930398},
  series = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  pages = {89--101},
  editor = {Dinh Phung and Tseng, {Vincent S.} and Geoff Webb and Bao Ho and Mohadeseh Ganji and Lida Rashidi},
  booktitle = {Advances in Knowledge Discovery and Data Mining},
  address = {Netherlands},
  note = {22nd Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2018), PAKDD 2018 ; Conference date: 03-06-2018 Through 06-06-2018},
  url = {http://prada-research.net/pakdd18/},
}

@article{8a56ff34e5bc4dada3bcc63d391de55e,
  title = {Automatically improving constraint models in Savile Row},
  abstract = {When solving a combinatorial problem using Constraint Programming (CP) or Satisfiability (SAT), modelling and formulation are vital and difficult tasks. Even an expert human may explore many alternatives in modelling a single problem. We make a number of contributions in the automated modelling and reformulation of constraint models. We study a range of automated reformulation techniques, finding combinations of techniques which perform particularly well together. We introduce and describe in detail a new algorithm, X-CSE, to perform Associative-Commutative Common Subexpression Elimination (AC-CSE) in constraint problems, significantly improving existing CSE techniques for associative and commutative operators such as +. We demonstrate that these reformulation techniques can be integrated in a single automated constraint modelling tool, called Savile Row, whose architecture we describe. We use Savile Row as an experimental testbed to evaluate each reformulation on a set of 50 problem classes, with 596 instances in total. Our recommended reformulations are well worthwhile even including overheads, especially on harder instances where solver time dominates. With a SAT solver we observed a geometric mean of 2.15 times speedup compared to a straightforward tailored model without recommended reformulations. Using a CP solver, we obtained a geometric mean of 5.96 times speedup for instances taking over 10 seconds to solve.},
  keywords = {Constraint satisfaction, Common subexpression elimination, Modelling, Reformulation, Propositional satisfiability},
  author = {Peter Nightingale and {\"O}zg{\"u}r Akg{\"u}n and Gent, {Ian P.} and Christopher Jefferson and Ian Miguel and Patrick Spracklen},
  note = {Authors thank the EPSRC for funding this work through grants EP/H004092/1, EP/K015745/1, EP/M003728/1, and EP/P015638/1. In addition, Dr Jefferson is funded by a Royal Society University Research Fellowship.},
  year = {2017},
  month = {oct},
  doi = {10.1016/j.artint.2017.07.001},
  language = {English},
  volume = {251},
  pages = {35--61},
  journal = {Artificial Intelligence},
  issn = {0004-3702},
  publisher = {Elsevier},
}

@conference{1b2ed603b33f4709870e2b38480f829d,
  title = {Learning From Past Links: Understanding the Limits of Linkage Quality},
  abstract = {The Digitising Scotland project aims to link 25 million vital event records from 1850s to 1970s. We aim to develop automatic approaches to probabilistic, similarity based record linkage. Linkage quality depends on the choices of keys and similarity measures. However, until now the effect of these choices has been unclear. We study the theoretical limits of automated linkage by performing a post-linkage analysis on two datasets, one from the Isle of Skye and one from Kilmarnock, previously linked by historical demographers. In these datasets, individuals appear on multiple certificates. The linkage problem involves unifying these occurrences e.g. between births and deaths, known as Entity Resolution. This requires the choice of particular keys, a similarity measure and a threshold signalling equivalence. We calculate linkage quality metrics–precision, recall, and F-measure–for 4 different key combinations, different similarity measures, and a range of threshold values. We present the distribution of similarity values for links and non-links for each configuration and data-set. From these results, we hope to understand the limits of automated probabilistic record linkage. We will use this understanding to inform our approach to the linkage of new unlinked datasets such as the Digitising Scotland dataset. We would welcome the opportunity to apply this approach to other linked demographic datasets.},
  author = {Ozgur Akgun and Alan Dearle and Eilidh Garrett and Kirby, {Graham Njal Cameron}},
  year = {2017},
  month = {sep},
  day = {6},
  language = {English},
  note = {British Society for Population Studies Annual Conference 2017, BSPS Annual Conference ; Conference date: 06-09-2017 Through 08-09-2017},
  url = {http://www.lse.ac.uk/socialPolicy/Researchcentresandgroups/BSPS/annualConference/Home.aspx},
}

@conference{b23bf2749f964504a25b98b76298e524,
  title = {Evaluating record linkage: creating longitudinal synthetic data to provide gold-standard linked data sets},
  abstract = {Gold-standard data to evaluate linkage algorithms are rare. Synthetic data have the advantage that all the true links are known. In the domain of population reconstruction, the ability to synthesise populations on demand, with varying characteristics, allows a linkage approach to be evaluated across a wide range of data sets.We present a micro-simulation model for generating such synthetic populations, taking as input a set of desired statistical properties. It then outlines how these desired properties are verified in the generated populations, and the intended approach to using generated populations to evaluate linkage algorithms. We envisage a sequence of experiments where a set of populations are generated to consider how linkage quality varies across different populations: with the same characteristics, with differing characteristics, and with differing types and levels of corruption. The performance of an approach at scale is also considered.},
  keywords = {record linkage},
  author = {Dalton, {Thomas Stanley} and Alan Dearle and Kirby, {Graham Njal Cameron} and Ozgur Akgun},
  year = {2017},
  month = {may},
  day = {11},
  language = {English},
  note = {Workshop for the Systematic Linking of Historical Records ; Conference date: 11-05-2017 Through 13-05-2017},
  url = {http://recordlink.org},
}

@conference{0f94ac46dc094dbfbf3e719ec7b8e43c,
  title = {Probabilistic linkage of vital event records in Scotland using familial groups},
  abstract = {We report on the assembly of longitudinal data from Scottish birth, death and marriage records representing eighteen million individuals. An experimental approach based on familial groups starts by gathering parents and their siblings into bundles with the aim of (as near of possible) partitioning the certificates into familial groups. This may be achieved by bundling marriage and birth certificates according to a signature derived from their attributes. This is similar to but different from blocking used in most entity resolution schemes where certificates of one kind are gathered together. We have experimented with these techniques using hand coded data from an historic Scottish dataset as a gold standard for comparison. In this paper we will report on our techniques and some preliminary results from our experiments.},
  keywords = {record linkage},
  author = {Ozgur Akgun and Dalton, {Thomas Stanley} and Alan Dearle and Eilidh Garrett and Kirby, {Graham Njal Cameron}},
  year = {2017},
  month = {may},
  day = {11},
  language = {English},
  note = {Workshop for the Systematic Linking of Historical Records ; Conference date: 11-05-2017 Through 13-05-2017},
  url = {http://recordlink.org},
}

@conference{421d8687dc4e4d64b23a7b8eeb63f63b,
  title = {An identifier scheme for the Digitising Scotland project},
  abstract = {The Digitising Scotland project is having the vital records of Scotland transcribed from images of the original handwritten civil registers . Linking the resulting dataset of 24 million vital records covering the lives of 18 million people is a major challenge requiring improved record linkage techniques. Discussions within the multidisciplinary, widely distributed Digitising Scotland project team have been hampered by the teams in each of the institutions using their own identification scheme. To enable fruitful discussions within the Digitising Scotland team, we required a mechanism for uniquely identifying each individual represented on the certificates. From the identifier it should be possible to determine the type of certificate and the role each person played. We have devised a protocol to generate for any individual on the certificate a unique identifier, without using a computer, by exploiting the National Records of Scotland•{\`A}_s registration districts. Importantly, the approach does not rely on the handwritten content of the certificates which reduces the risk of the content being misread resulting in an incorrect identifier. The resulting identifier scheme has improved the internal discussions within the project. This paper discusses the rationale behind the chosen identifier scheme, and presents the format of the different identifiers. The work reported in the paper was supported by the British ESRC under grants ES/K00574X/1(Digitising Scotland) and ES/L007487/1 (Administrative Data Research Center - Scotland).},
  keywords = {record linkage},
  author = {Ozgur Akgun and Ahmad Al-Sidiqi and Peter Christen and Dalton, {Thomas Stanley} and Alan Dearle and Dibben, {Christopher John Lloyd} and Eilidh Garrett and Alasdair Gray and Kirby, {Graham Njal Cameron} and Alice Reid},
  year = {2017},
  month = {apr},
  day = {2},
  language = {English},
  note = {UK Administrative Data Research Network Annual Research Conference : Social science using administrative data for public benefit, ADRN2017 ; Conference date: 01-06-2017 Through 02-06-2017},
  url = {http://www.adrn2017.net},
}

@conference{2c922d20ebd541c7a1673a55e379b693,
  title = {Evaluating population data linkage: assessing stability, scalability, resilience and robustness across many data sets for comprehensive linkage evaluation},
  abstract = {Data linkage approaches are often evaluated with small or few data sets. If a linkage approach is to be used widely, quantifying its performance with varying data sets would be beneficial. In addition, given a data set needs to be linked, the true links are by definition unknown. The success of a linkage approach is thus difficult to comprehensively evaluate. This talk focuses on the use of many synthetic data sets for the evaluation of linkage quality achieved by automatic linkage algorithms in the domain of population reconstruction. It presents an evaluation approach which considers linkage quality when characteristics of the population are varied. We envisage a sequence of experiments where a set of populations are generated to consider how linkage quality varies across different populations: with the same characteristics, with differing characteristics, and with differing types and levels of corruption. The performance of an approach at scale is also considered. The approach to generate synthetic populations with varying characteristics on demand will also be addressed. The use of synthetic populations has the advantage that all the true links are known, thus allowing evaluation as if with real-world 'gold-standard' linked data sets. Given the large number of data sets evaluated against we also give consideration as to how to present these findings. The ability to assess variations in linkage quality across many data sets will assist in the development of new linkage approaches and identifying areas where existing linkage approaches may be more widely applied.},
  keywords = {data linkage},
  author = {Dalton, {Thomas Stanley} and Ozgur Akgun and Ahmad Al-Sediqi and Peter Christen and Alan Dearle and Eilidh Garrett and Alasdair Gray and Kirby, {Graham Njal Cameron} and Alice Reid},
  year = {2017},
  month = {apr},
  day = {2},
  language = {English},
  note = {UK Administrative Data Research Network Annual Research Conference : Social science using administrative data for public benefit, ADRN2017 ; Conference date: 01-06-2017 Through 02-06-2017},
  url = {http://www.adrn2017.net},
}

@conference{5caabedd32764dbbbe8c81724c8cd9e2,
  title = {Record linking using metric space similarity search},
  abstract = {Record linking often employs blocking to reduce the computational complexity of full pairwise comparison. A key is formed from a subset of record attributes. Those records with the same key values are blocked together for detailed comparison. Use of a single blocking key fails to detect many true matches if records contain missing values or errors, since only those records with the same key values are compared. To address missing values, it is common to repeat the matching process using multiple blocking keys, to match records that are identical in a subset of the fields. The presence of erroneous values may be addressed by blocking using key values mapped to a canonical form (e.g. Soundex). However, this does not address other problems such as single digit transcription errors in dates.Blocking is used to categorise records that are candidate matches, in preparation for a pairwise comparison phase which may use various distance metrics, depending on the domain of the values being compared. Each blocking process defines a partition of records. The comparison operations are only applied to pairs of records within the same category.In some contexts, it may be useful to have flexible control over the precision/recall trade-off, depending on the intended use for the matched data, and the degree of conservatism required of the identified links. With blocking, this flexibility is limited by the number of sensible blocking keys that can be identified.In this talk, we describe experiments with a technique based on similarity searching over metric spaces, which appears to offer greater flexibility, and describe some preliminary results using an historic Scottish dataset. },
  keywords = {record linkage},
  author = {Alan Dearle and Kirby, {Graham Njal Cameron} and Ozgur Akgun and Dalton, {Thomas Stanley}},
  year = {2017},
  month = {apr},
  day = {2},
  language = {English},
  note = {UK Administrative Data Research Network Annual Research Conference : Social science using administrative data for public benefit, ADRN2017 ; Conference date: 01-06-2017 Through 02-06-2017},
  url = {http://www.adrn2017.net},
}

@inproceedings{03feab994c4544a49e1387c269b809e0,
  title = {Exploiting short supports for improved encoding of arbitrary constraints into SAT},
  abstract = {Encoding to SAT and applying a highly efficient modern SAT solver is an increasingly popular method of solving finite-domain constraint problems. In this paper we study encodings of arbitrary constraints where unit propagation on the encoding provides strong reasoning. Specifically, unit propagation on the encoding simulates generalised arc consistency on the original constraint. To create compact and efficient encodings we use the concept of short support. Short support has been successfully applied to create efficient propagation algorithms for arbitrary constraints. A short support of a constraint is similar to a satisfying tuple however a short support is not required to assign every variable in scope. Some variables are left free to take any value. In some cases a short support representation is smaller than the table of satisfying tuples by an exponential factor. We present two encodings based on short supports and evaluate them on a set of benchmark problems, demonstrating a substantial improvement over the state of the art. },
  author = {{\"O}zg{\"u}r Akg{\"u}n and Gent, {Ian Philip} and Jefferson, {Christopher Anthony} and Miguel, {Ian James} and Nightingale, {Peter William}},
  year = {2016},
  doi = {10.1007/978-3-319-44953-1_1},
  language = {English},
  isbn = {9783319449524},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  pages = {3--12},
  editor = {Michael Rueher},
  booktitle = {Twenty-Second International Conference on Principles and Practice of Constraint Programming (CP 2016)},
  address = {Netherlands},
  note = {22nd International Conference on Principles and Practice of Constraint Programming (CP 2016) ; Conference date: 05-09-2016 Through 09-09-2016},
}

@inproceedings{3881455b36054960a6bae0910108ab95,
  title = {Cloud-based e-Infrastructure for scheduling astronomical observations},
  abstract = {Gravitational microlensing exploits a transient phenomenon where an observed star is brightened due to deflection of its light by the gravity of an intervening foreground star. It is conjectured that this technique can be used to measurethe abundance of planets throughout the Milky Way. In order to undertake efficient gravitational microlensing an observation schedule must be constructed such that various targets are observed while undergoing a microlensing event. In this paper, we propose a cloud-based e-Infrastructure that currently supportsfour methods to compute candidate schedules via the application of local search and probabilistic meta-heuristics. We then validate the feasibility of the e-Infrastructure by evaluating the methods on historic data. The experiments demonstrate that the use of on-demand cloud resources for the e-Infrastructure can allow better schedules to be found more rapidly.},
  author = {Wetter, {James Patrick} and Ozgur Akgun and Barker, {Adam David} and Martin Dominik and Miguel, {Ian James} and Blesson Varghese},
  note = {This research was pursued under the EPSRC grant Working Together: Constraint Programming and Cloud Computing (EP/K015745/1) and an Amazon Web Services (AWS) Education Research Grant. ; 11th IEEE International Conference on eScience ; Conference date: 31-08-2015 Through 04-09-2015},
  year = {2015},
  month = {aug},
  day = {31},
  doi = {10.1109/eScience.2015.54},
  language = {English},
  pages = {362--370},
  booktitle = {2015 IEEE 11th International Conference on e-Science (e-Science) (2015)},
  publisher = {IEEE Computer Society},
  address = {United States},
  url = {http://escience2015.mnm-team.org/},
}

@inproceedings{9dd26cea0c54476f8d0418df430909da,
  title = {Automatically generating streamlined constraint models with ESSENCE and CONJURE},
  abstract = {Streamlined constraint reasoning is the addition of uninferred constraints to a constraint model to reduce the search space, while retaining at least one solution. Previously, effective streamlined models have been constructed by hand, requiring an expert to examine closely solutions to small instances of a problem class and identify regularities. We present a system that automatically generates many conjectured regularities for a given Essence specification of a problem class by examining the domains of decision variables present in the problem specification. These conjectures are evaluated independently and in conjunction with one another on a set of instances from the specified class via an automated modelling tool-chain comprising of Conjure, Savile Row and Minion. Once the system has identified effective conjectures they are used to generate streamlined models that allow instances of much larger scale to be solved. Our results demonstrate good models can be identified for problems in combinatorial design, Ramsey theory, graph theory and group theory - often resulting in order of magnitude speed-ups.},
  author = {James Wetter and Ozgur Akgun and Ian Miguel},
  year = {2015},
  month = {aug},
  day = {13},
  doi = {10.1007/978-3-319-23219-5_34},
  language = {English},
  isbn = {9783319232188},
  series = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  publisher = {Springer},
  pages = {480--496},
  editor = {Gilles Pesant},
  booktitle = {Twenty-First International Conference on Principles and Practice of Constraint Programming (CP 2015)},
  address = {Netherlands},
  note = {21st International Conference on the Principles and Practice of Constraint Programming, CP 2015 ; Conference date: 31-08-2015 Through 04-09-2015},
}

@inproceedings{0e8ec1d427b148acb1f0d20b9ef51659,
  title = {Cloud benchmarking for performance},
  abstract = {How can applications be deployed on the cloud to achieve maximum performance? This question has become significant and challenging with the availability of a wide variety of Virtual Machines (VMs) with different performance capabilities in the cloud. The above question is addressed by proposing a six step benchmarking methodology in which a user provides a set of four weights that indicate how important each of the following groups: memory, processor, computation and storage are to the application that needs to be executed on the cloud. The weights along with cloud benchmarking data are used to generate a ranking of VMs that can maximise performance of the application. The rankings are validated through an empirical analysis using two case study applications, the first is a financial risk application and the second is a molecular dynamics simulation, which are both representative of workloads that can benefit from execution on the cloud. Both case studies validate the feasibility of the methodology and highlight that maximum performance can be achieved on the cloud by selecting the top ranked VMs produced by the methodology.},
  author = {Blesson Varghese and Ozgur Akgun and Ian Miguel and Long Thai and Adam Barker},
  year = {2014},
  month = {dec},
  day = {15},
  doi = {10.1109/CloudCom.2014.28},
  language = {English},
  isbn = {9781479940936},
  pages = {535--540},
  booktitle = {Proceedings 2014 IEEE 6th International Conference on Cloud Computing Technology and Science (CloudCom 2014)},
  publisher = {IEEE},
  note = {6th IEEE International Conference on Cloud Computing Technology and Science (CloudCom 2014), CloudCom 2014 ; Conference date: 15-12-2014 Through 18-12-2014},
}

@inproceedings{09b16cf13ed94fb1b2fed6296b5a75d1,
  title = {Optimal deployment of geographically distributed workflow engines on the Cloud},
  abstract = {When orchestrating Web service workflows, the geographical placement of the orchestration engine(s) can greatly affect workflow performance. Data may have to be transferred across long geographical distances, which in turn increases execution time and degrades the overall performance of a workflow. In this paper, we present a framework that, given a DAG-based workflow specification, computes the op- timal Amazon EC2 cloud regions to deploy the orchestration engines and execute a workflow. The framework incorporates a constraint model that solves the workflow deployment problem, which is generated using an automated constraint modelling system. The feasibility of the framework is evaluated by executing different sample workflows representative of sci- entific workloads. The experimental results indicate that the framework reduces the workflow execution time and provides a speed up of 1.3x-2.5x over centralised approaches.},
  keywords = {Workflow engine, Optimal deployment, Cloud computing, Workflow execution},
  author = {Long Thai and Adam Barker and Blesson Varghese and Ozgur Akgun and Ian Miguel},
  note = {This research was pursued under the EPSRC Working Together: Constraint Programming and Cloud Computing grant, a Royal Society Industry Fellowship Bringing Science to the Cloud, and an Amazon Web Services Education Research Grant. Date of Acceptance: 02/09/2014},
  year = {2014},
  month = {oct},
  day = {30},
  doi = {10.1109/CloudCom.2014.30},
  language = {English},
  isbn = {9781479940936 },
  pages = {811--816},
  booktitle = {6th IEEE International Conference on Cloud Computing Technology and Science (CloudCom 2014)},
  publisher = {IEEE},
}

@inproceedings{4d61e32b389247fe849054856b224aa1,
  title = {Automatically improving constraint models in Savile Row through associative-commutative common subexpression elimination},
  abstract = {When solving a problem using constraint programming, constraint modelling is widely acknowledged as an important and difficult task. Even a constraint modelling expert may explore many models and spend considerable time modelling a single problem. Therefore any automated assistance in the area of constraint modelling is valuable. Common sub-expression elimination (CSE) is a type of constraint reformulation that has proved to be useful on a range of problems. In this paper we demonstrate the value of an extension of CSE called Associative-Commutative CSE (AC-CSE). This technique exploits the properties of associativity and commutativity of binary operators, for example in sum constraints. We present a new algorithm, X-CSE, that is able to choose from a larger palette of common subexpressions than previous approaches. We demonstrate substantial gains in performance using X-CSE. For example on BIBD we observed speed increases of more than 20 times compared to a standard model and that using X-CSE outperforms a sophisticated model from the literature. For Killer Sudoku we found that X-CSE can render some apparently difficult instances almost trivial to solve, and we observe speed increases up to 350 times. For BIBD and Killer Sudoku the common subexpressions are not present in the initial model: an important part of our methodology is reformulations at the preprocessing stage, to create the common subexpressions for X-CSE to exploit. In summary we show that X-CSE, combined with preprocessing and other reformulations, is a powerful technique for automated modelling of problems containing associative and commutative constraints.},
  keywords = {Symmetry, Breaking, System},
  author = {Peter Nightingale and Ozgur Akgun and Gent, {Ian P.} and Christopher Jefferson and Ian Miguel},
  note = {We would like to thank the Royal Society for funding through Dr Jeffersons URF, and the EPSRC for funding this work through grant EP/H004092/1.; 20th International Conference on the Principles and Practice of Constraint Programming (CP 2014), CP 2014 ; Conference date: 08-09-2014 Through 12-09-2014},
  year = {2014},
  month = {sep},
  day = {8},
  doi = {10.1007/978-3-319-10428-7_43},
  language = {English},
  isbn = {9783319104270},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  pages = {590--605},
  editor = {Barry O'Sullivan},
  booktitle = {Twentieth International Conference on Principles and Practice of Constraint Programming (CP 2014)},
  address = {Netherlands},
}

@inbook{dd9347655a2d45f2bef81ebcb8778daa,
  title = {Breaking conditional symmetry in automated constraint modelling with CONJURE},
  abstract = {Many constraint problems contain symmetry, which can lead to redundant search. If a partial assignment is shown to be invalid, we are wasting time if we ever consider a symmetric equivalent of it. A particularly important class of symmetries are those introduced by the constraint modelling process: model symmetries. We present a systematic method by which the automated constraint modelling tool CONJURE can break conditional symmetry as it enters a model during refinement. Our method extends, and is compatible with, our previous work on automated symmetry breaking in CONJURE. The result is the automatic and complete removal of model symmetries for the entire problem class represented by the input specification. This applies to arbitrarily nested conditional symmetries and represents a significant step forward for automated constraint modelling.},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Ian Gent and Chris Jefferson and Ian Miguel and Peter Nightingale},
  note = {This work was supported by UK EPSRC EP/K015745/1. Jefferson is supported by a Royal Society University Research Fellowship.},
  year = {2014},
  month = {aug},
  day = {18},
  doi = {10.3233/978-1-61499-419-0-3},
  language = {English},
  isbn = {9781614994183},
  volume = {263},
  series = {Frontiers in Artificial Intelligence and Applications},
  publisher = {IOS Press},
  pages = {3--8},
  editor = {Torsten Schaub and Gerhard Friedrich and Barry O'Sullivan},
  booktitle = {ECAI 2014},
  address = {Netherlands},
}

@misc{0ef98e79e0bd426eb92227f281f4ee4e,
  title = {Extensible automated constraint modelling via refinement of abstract problem specifications},
  abstract = {Constraint Programming (CP) is a powerful technique for solving large-scale combinatorial (optimisation) problems. Constraint solving a given problem proceeds in two phases: modelling and solving. Effective modelling has an huge impact on the performance of the solving process. This thesis presents a framework in which the users are not required to make modelling decisions, concrete CP models are automatically generated from a high level problem specification. In this framework, modelling decisions are encoded as generic rewrite rules applicable to many different problems. First, modelling decisions are divided into two broad categories. This categorisation guides the automation of each kind of modelling decision and also leads us to the architecture of the automated modelling tool. Second, a domain-specific declarative rewrite rule language is introduced. Thanks to the rule language, automated modelling transformations and the core system are decoupled. The rule language greatly increases the extensibility and maintainability of the rewrite rules database. The database of rules represents the modelling knowledge acquired after analysis of expert models. This database must be easily extensible to best benefit from the active research on constraint modelling. Third, the automated modelling system Conjure is implemented as a realisation of these ideas; having an implementation enables empirical testing of the quality of generated models. The ease with which rewrite rules can be encoded to produce good models is shown. Furthermore, thanks to the generality of the system, one needs to add a very small number of rules to encode many transformations. Finally, the work is evaluated by comparing the generated models to expert models found in the literature for a wide variety of benchmark problems. This evaluation confirms the hypothesis that expert models can be automatically generated starting from high level problem specifications. An method of automatically identifying good models is also presented. In summary, this thesis presents a framework to enable the automatic generation of efficient constraint models from problem specifications. It provides a pleasant environment for both problem owners and modelling experts. Problem owners are presented with a fully automated constraint solution process, once they have a precise description of their problem. Modelling experts can now encode their precious modelling expertise as rewrite rules instead of merely modelling a single problem; resulting in reusable constraint modelling knowledge.},
  author = {Ozgur Akgun},
  year = {2014},
  language = {English},
  publisher = {University of St Andrews},
  url = {http://hdl.handle.net/10023/6547},
}

@inproceedings{ec7a3b357c8b4af4ba335c281ea87ba3,
  title = {An Automated Constraint Modelling and Solving Toolchain},
  author = {Ozgur Akgun and Frisch, {Alan M} and Gent, {Ian Philip} and Hussain, {Bilal Syed} and Jefferson, {Christopher Anthony} and Lars Kotthoff and Miguel, {Ian James} and Nightingale, {Peter William}},
  year = {2013},
  language = {English},
  booktitle = {ARW 2013 - 20th Automated Reasoning Workshop},
  url = {http://staff.computing.dundee.ac.uk/katya/arw13/papers/paper_13.pdf},
}

@inproceedings{dfc0e4b721844d9d801fb27c264086ff,
  title = {Automated Modelling and Model Selection in Constraint Programming: Current Achievements and Future Directions},
  abstract = {In attacking the modelling bottleneck, we present current achievements in automated model generation and selection in constraint programming (CP). We also discuss promising future directions in automated model selection, which we believe are of key importance in enabling successful automated modelling in CP.},
  author = {Ozgur Akgun and Frisch, {Alan M} and Jefferson, {Christopher Anthony} and Miguel, {Ian James}},
  year = {2013},
  language = {English},
  booktitle = {COSpeL: The first Workshop on Domain Specific Languages in Combinatorial Optimization},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.388.5906&rep=rep1&type=pdf},
}

@inproceedings{066dfdbd563a40c68df2eaae83a342cd,
  title = {Automated Symmetry Breaking and Model Selection in Conjure},
  abstract = {Constraint modelling is widely recognised as a key bottleneck in applying constraint solving to a problem of interest. The Conjure automated constraint modelling system addresses this problem by automatically refining constraint models from problem specifications written in the Essence language. Essence provides familiar mathematical concepts like sets, functions and relations nested to any depth. To date, Conjure has been able to produce a set of alternative model kernels (i.e. without advanced features such as symmetry breaking or implied constraints) for a given specification. The first contribution of this paper is a method by which Conjure can break symmetry in a model as it is introduced by the modelling process. This works at the problem class level, rather than just individual instances, and does not require an expensive detection step after the model has been formulated. This allows Conjure to produce a higher quality set of models. A further limitation of Conjure has been the lack of a mechanism to select among the models it produces. The second contribution of this paper is to present two such mechanisms, allowing effective models to be chosen automatically.},
  author = {Ozgur Akgun and Frisch, {Alan M} and Gent, {Ian Philip} and Hussain, {Bilal Syed} and Jefferson, {Christopher Anthony} and Lars Kotthoff and Miguel, {Ian James} and Nightingale, {Peter William}},
  year = {2013},
  doi = {10.1007/978-3-642-40627-0_11},
  language = {English},
  booktitle = {Nineteenth International Conference on Principles and Practice of Constraint Programming (CP 2013)},
}

@inproceedings{ed9a3c921f40425b964217636a5af521,
  title = {Extensible Automated Constraint Modelling},
  abstract = {In constraint solving, a critical bottleneck is the formulation of an effective constraint model of a given problem. The CONJURE system described in this paper, a substantial step forward over prototype versions of CONJURE previously reported, makes a valuable contribution to the automation of constraint modelling by automatically producing constraint models from their specifications in the abstract constraint specification language ESSENCE. A set of rules is used to refine an abstract specification into a concrete constraint model. We demonstrate that this set of rules is readily extensible to increase the space of possible constraint models CONJURE can produce. Our empirical results confirm that CONJURE can reproduce successfully the kernels of the constraint models of 32 benchmark problems found in the literature.},
  author = {Ozgur Akgun and Miguel, {Ian James} and Jefferson, {Christopher Anthony} and Frisch, {Alan M.} and Brahim Hnich},
  year = {2011},
  language = {English},
  isbn = {978-157735508-3},
  pages = {4--11},
  booktitle = {Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence},
  publisher = {AAAI Press},
  url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3687},
  note = {25th AAAI Conference on Artificial Intelligence and the 23rd Innovative Applications of Artificial Intelligence Conference, AAAI-11 / IAAI-11 ; Conference date: 07-08-2011 Through 11-08-2011},
}

@inproceedings{ecb8ca42a8eb4098a685daa84c821da3,
  title = {The Open Stacks Problem: An automated modelling case study},
  author = {Ozgur Akgun and Miguel, {Ian James} and Jefferson, {Christopher Anthony}},
  year = {2011},
  language = {English},
  pages = {15},
  booktitle = {ERCIM Workshop on Constraint Solving and Constraint Logic Programming},
  url = {https://csclp2011.cs.st-andrews.ac.uk/csclp2011proceedings.pdf#page=21},
}

@inbook{aee46a0c83b14b25b687813677855191,
  title = {Conjure Revisited: Towards Automated Constraint Modelling},
  abstract = {Automating the constraint modelling process is one of thekey challenges facing the constraints field, and one of the principal obstaclespreventing widespread adoption of constraint solving. This paperfocuses on the refinement-based approach to automated modelling, wherea user specifies a problem in an abstract constraint specification languageand it is then automatically refined into a constraint model. In particular,we revisit the Conjure system that first appeared in prototype formin 2005 and present a new implementation with a much greater coverageof the specification language Essence},
  author = {{\"O}zg{\"u}r Akg{\"u}n and Frisch, {Alan M} and Brahim Hnich and Jefferson, {Christopher Anthony} and Miguel, {Ian James}},
  year = {2010},
  language = {English},
  booktitle = {ModRef 2010 - The 9th International Workshop on Constraint Modelling and Reformulation},
  url = {https://it.uu.se/research/group/astra/ModRef10/papers/Ozgur%20Akgun,%20Alan%20Frisch,%20Brahim%20Hnich,%20Chris%20Jefferson%20and%20Ian%20Miguel.%20%20Conjure%20Revisited,%20Towards%20Automated%20Constraint%20Modelling%20-%20ModRef%202010.pdf},
}

@inproceedings{586c88ee08c1404391bc48c26d66e523,
  title = {Refining Portfolios of Constraint Models with Conjure},
  abstract = {Modelling is one of the key challenges in Constraint Programming(CP). There are many ways in which to model a given problem.The model chosen has a substantial effect on the solving efficiency. Itis difficult to know what the best model is. To overcome this problem wetake a portfolio approach: Given a high level specification of a combinatorialproblem, we employ non-deterministic rewrite techniques to obtaina portfolio of constraint models. The specification language (Essence)does not require humans to make modelling decisions; therefore it helpsus remove the modelling bottleneck.},
  author = {Ozgur Akgun},
  year = {2010},
  language = {English},
  pages = {1--6},
  booktitle = {CP 2010 - Principles and Practice of Constraint Programming, 16th International Conference, Doctoral Program},
  url = {https://s3.amazonaws.com/academia.edu.documents/30783376/DP10Proceedings.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1533056575&Signature=x2Slk0PdJ3tZOc%2FvWV%2FHOkIGgCc%3D&response-content-disposition=inline%3B%20filename%3DRefining_Portfolios_of_Constraint_Models.pdf#page=6},
}


